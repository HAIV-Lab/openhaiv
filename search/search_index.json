{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udc4b Introduction","text":"<p>The OpenHAIV framework primarily focuses on Open World Learing and currently supports the following tasks.</p>"},{"location":"#supported-tasks","title":"\ud83c\udff7\ufe0f Supported Tasks","text":""},{"location":"#supervised-learning","title":"\ud83c\udfaf Supervised Learning","text":"<p>Supervised learning is one of the most commonly used machine learning algorithms. In the field of computer vision, tasks such as image classification typically rely on labeled data for training. Labeled data refers to paired input data and corresponding target outputs (labels). Through these pairs, the model learns the mapping from input to output. During training, the model continuously adjusts its internal parameters to minimize the gap between predictions and actual labels. A fundamental training framework should support supervised learning, and Openhaiv allows the use of different deep learning models, datasets, and flexible hyperparameter tuning.</p>"},{"location":"#incremental-learning","title":"\ud83c\udf31 Incremental Learning","text":"<p>Incremental learning is an advanced machine learning paradigm designed to address the issue of catastrophic forgetting in machine learning. Specifically, incremental learning enables models to learn new knowledge while retaining and optimizing previously acquired knowledge. Currently, mainstream incremental learning methods can be categorized into three types: regularization-based methods, experience replay-based methods, and parameter isolation-based methods. The Openhaiv framework currently supports multiple incremental learning algorithms, including ALICE, FACT, and SAVC, allowing users to select and adjust them based on task requirements.</p>"},{"location":"#few-shot-learning","title":"\ud83e\udde9 Few-Shot Learning","text":"<p>Few-shot learning is a common scenario in deep learning, where models must learn and make inferences effectively with only a small number of samples. Specifically, few-shot learning uses a limited number of samples during training (typically 5\u201310 samples per class). The OpenHAIV framework currently supports few-shot learning, allowing users to define models, datasets, and the number of samples per class in the few-shot stage. Few-shot learning often employs methods such as contrastive learning and augmentation techniques, which are well-supported in Openhaiv. Additionally, the framework is highly extensible, enabling the integration of new few-shot learning methods.</p>"},{"location":"#out-of-distribution-detection","title":"\ud83d\udea8 Out-of-Distribution Detection","text":"<p>Out-of-distribution detection is the process of identifying whether data deviates from the known data distribution. On one hand, OOD detection ensures that models can make reliable judgments when encountering inputs different from known classes, thereby improving system robustness. On the other hand, OOD detection is a crucial component of incremental object recognition in open environments. The Openhaiv framework currently supports multiple OOD detection algorithms, allowing flexible adjustments for different domains and tasks.</p>"},{"location":"#novel-category-discovery","title":"\ud83d\udd0d Novel Category Discovery","text":"<p>Novel category discovery refers to identifying and discovering new categories in data without pre-existing labeled classes. Currently, novel category discovery typically employs unsupervised clustering algorithms in machine learning. Unsupervised clustering groups datasets into clusters, and Openhaiv supports the use of the K-Means clustering algorithm for novel category discovery. K-Means partitions data into K clusters, with each cluster represented by its centroid (the center point of the cluster). The algorithm iteratively updates centroids to minimize the within-cluster squared distances.</p>"},{"location":"#key-features","title":"\u2b50 Key Features","text":""},{"location":"#modular-design","title":"\ud83e\udde9 Modular Design","text":"<p>Each component can be independently configured, tested, and replaced, enabling:</p> <ul> <li>Easy experimentation with different algorithms</li> <li>Seamless integration of new techniques</li> <li>Flexible deployment configurations</li> </ul>"},{"location":"#scalable-architecture","title":"\ud83d\ude80 Scalable Architecture","text":"<p>The framework supports:</p> <ul> <li>Large-scale dataset processing</li> <li>Distributed training capabilities</li> <li>Efficient memory management</li> <li>Parallel processing optimization</li> </ul>"},{"location":"#extensible-framework","title":"\ud83d\udd0c Extensible Framework","text":"<p>Built for research and production use:</p> <ul> <li>Plugin-based algorithm integration</li> <li>Custom loss function support</li> <li>Flexible evaluation metrics</li> <li>Comprehensive logging and monitoring</li> </ul>"},{"location":"acknowledgment/","title":"\ud83d\ude4f Acknowledgment","text":"<ul> <li>OpenOOD, an extensible codebase for out-of-distribution detection with Vision Models only.</li> <li>OpenOOD-VLM, an extensible codebase for out-of-distribution detection with both Vision Models and Vision-Language Models.</li> <li>PyCIL, an extensible codebase for incremental learning.</li> </ul>"},{"location":"citation/","title":"\ud83d\udcd6 Citation","text":"<p>If you find our repository useful for your research, please consider citing these papers: BibTeX<pre><code>@article{xiang2025openhaiv,\n  title={OpenHAIV: A Framework Towards Practical Open-World Learning},\n  author={Xiang, Xiang and Zhou, Qinhao and Xu, Zhuo and Ma, Jing and Dai, Jiaxin and Liang, Yifan and Li, Hanlin},\n  journal={arXiv preprint arXiv:2508.07270},\n  year={2025},\n  url={https://arxiv.org/abs/2508.07270},\n  doi={10.48550/arXiv.2508.07270}\n}\n</code></pre></p>"},{"location":"config/","title":"\u2699\ufe0f Config","text":"<p>The OpenHAIV framework uses a flexible YAML-based configuration system that allows users to easily define and modify various experimental parameters without changing the code. This document provides detailed information about the structure, usage, and major components of the configuration files.</p>"},{"location":"config/#configuration-file-structure","title":"Configuration File Structure","text":"<p>OpenHAIV configuration files follow a hierarchical structure, mainly divided into the following sections:</p> <ol> <li>Base Configuration: Defines basic parameters for experiments, such as random seed, device, output directory, etc.</li> <li>Trainer Configuration: Defines parameters for the training process</li> <li>Model Configuration: Defines model architecture and related parameters</li> <li>Algorithm Configuration: Defines specific algorithms and methods</li> <li>Data Loader Configuration: Defines datasets and data loading parameters</li> <li>Optimizer Configuration: Defines optimization algorithms and related parameters</li> <li>Scheduler Configuration: Defines learning rate scheduling strategies</li> </ol> <p>Configuration files can inherit from other configuration files through the <code>_base_</code> field, enabling modular and reusable configurations.</p>"},{"location":"config/#configuration-modules-in-detail","title":"Configuration Modules in Detail","text":""},{"location":"config/#configalgorithms","title":"config.algorithms","text":"<p>Algorithm configurations define the specific algorithms and methods used in experiments, including:</p>"},{"location":"config/#supervised-learning-algorithms","title":"Supervised Learning Algorithms","text":"YAML<pre><code>algorithm:\n  type: StandardSL  # Standard supervised learning\n</code></pre>"},{"location":"config/#class-incremental-learning-algorithms","title":"Class-incremental Learning Algorithms","text":"YAML<pre><code>algorithm:\n  type: LwF  # Learning without Forgetting\n  # Algorithm-specific parameters\n  temperature: 2.0\n  alpha: 1.0\n</code></pre> <p>Other supported class-incremental learning algorithms include: <code>Finetune</code>, <code>Joint</code>, <code>iCaRL</code>, <code>EWC</code>, <code>BiC</code>, <code>WA</code>, <code>DER</code>, <code>Coil</code>, <code>FOSTER</code>, <code>SSRE</code>, <code>FeTrIL</code>, <code>MEMO</code>, etc.</p>"},{"location":"config/#few-shot-class-incremental-learning-algorithms","title":"Few-shot Class-incremental Learning Algorithms","text":"YAML<pre><code>algorithm:\n  type: SAVC  # Few-shot class-incremental learning algorithm\n  # Algorithm-specific parameters\n  alpha: 1.0\n  beta: 0.1\n</code></pre>"},{"location":"config/#out-of-distribution-detection-algorithms","title":"Out-of-Distribution Detection Algorithms","text":"YAML<pre><code>algorithm:\n  type: MSP  # Maximum Softmax Probability\n  # or\n  type: MLS  # Maximum Logit Score\n  # or\n  type: MCM  # Maximum Concept Matching\n</code></pre> <p>Other supported out-of-distribution detection algorithms include: <code>ODIN</code>, <code>VIM</code>, <code>MDS</code>, <code>DML</code>, <code>FDBD</code>, <code>GODIN</code>, <code>MCM</code>, <code>GL-MCM</code>, etc.</p>"},{"location":"config/#configdataloader","title":"config.dataloader","text":"<p>Data loader configurations define the datasets and data loading parameters used in experiments:</p> YAML<pre><code>_base_: configs/dataloader/OES/oes.yaml  # Inherit base dataset configuration\n\ndataloader:\n  train:\n    dataset:\n      type: OES  # Dataset type\n      root: /path/to/dataset  # Dataset path\n      split: train  # Dataset split\n    batch_size: 128  # Batch size\n    num_workers: 4  # Number of data loading worker threads\n    shuffle: True  # Whether to shuffle data\n\n  val:  # Validation set configuration\n    dataset:\n      type: OES\n      root: /path/to/dataset\n      split: val\n    batch_size: 128\n    num_workers: 4\n    shuffle: False\n</code></pre> <p>Supported datasets include: <code>OES</code>, <code>CIFAR10</code>, <code>CIFAR100</code>, <code>ImageNet</code>, <code>ImageNetR</code>, <code>CUB200</code>, <code>Remote</code>, <code>Food101</code>, <code>Caltech101</code>, etc.</p>"},{"location":"config/#configmodel","title":"config.model","text":"<p>Model configurations define the model architecture and related parameters used in experiments:</p> YAML<pre><code>model:\n  type: ResNet18  # Model type\n  # Or use a more complex definition\n  type: ResNet_Base\n  network:\n    type: ResNet18\n    num_classes: 94\n  checkpoint: \"/path/to/checkpoint.pth\"  # Pre-trained model path\n  loss: CrossEntropyLoss  # Loss function type\n</code></pre> <p>Supported models include: - Standard CNN models: <code>ResNet18</code>, <code>ResNet34</code>, <code>ResNet50</code>, <code>ResNet101</code> - Vision-language models: <code>CLIP-B/16</code>, <code>CLIP-B/32</code>, <code>RSCLIP-B/16</code>, <code>RSCLIP-B/32</code> - Algorithm-specific models: <code>AliceNET</code>, <code>CoOp</code>, <code>LoCoOp</code>, <code>SCT</code>, <code>DPM</code></p>"},{"location":"config/#configpipeline","title":"config.pipeline","text":"<p>Pipeline configuration files integrate all of the above components into a complete experiment configuration, for example, a supervised learning pipeline configuration:</p> YAML<pre><code>_base_: configs/dataloader/OES/oes.yaml  # Inherit data loader configuration\n\ntrainer:\n  type: PreTrainer  # Trainer type\n  max_epochs: 10  # Maximum training epochs\n\nalgorithm:\n  type: StandardSL  # Algorithm type\n\nmodel:\n  type: ResNet18  # Model type\n\ncriterion:\n  type: CrossEntropyLoss  # Loss function\n\noptimizer:\n  type: Adam  # Optimizer\n  lr: 0.0003  # Learning rate\n  weight_decay: 0.0001  # Weight decay\n\nscheduler:\n  type: Constant  # Scheduler type\n\nseed: 0  # Random seed\ndevice: cuda  # Device\n\nexp_name: exp  # Experiment name\nwork_dir: ./output/supervised/sl_oes_rn18  # Output directory\n</code></pre>"},{"location":"config/#how-to-use-configurations","title":"How to Use Configurations","text":"<p>Configuration files can be passed to the training script via command line:</p> Bash<pre><code>python ncdia/train.py --cfg configs/pipeline/supervised_learning/sl_oes_rn18.yaml --opts device='cuda:0'\n</code></pre> <p>The <code>--opts</code> parameter allows overriding any parameter in the configuration file.</p>"},{"location":"config/#configuration-inheritance-mechanism","title":"Configuration Inheritance Mechanism","text":"<p>OpenHAIV's configuration system supports composition based on inheritance, making configurations more modular and reusable:</p> YAML<pre><code>_base_: [\n  configs/dataloader/OES/oesfull.yaml,  # Inherit data loader configuration\n  configs/algorithms/ood_detection/dml.yaml,  # Inherit algorithm configuration\n]\n\n# Override or add specific parameters\ntrainer:\n  type: DetTrainer\n  max_epochs: 10\n\nmodel:\n  type: ResNet_Base\n  network:\n    type: ResNet18\n    num_classes: 94\n</code></pre> <p>Through the inheritance mechanism, repetitive configurations can be avoided, improving the maintainability of configuration files.</p>"},{"location":"config/#configuration-best-practices","title":"Configuration Best Practices","text":"<p>Note</p> <p>Please follow these best practices when using configuration files in OpenHAIV:</p> <ol> <li>Modular Configuration: Extract common configurations as base configuration files, and combine them through inheritance</li> <li>Use Relative Paths: Paths in configuration files should use paths relative to the project root directory when possible</li> <li>Comment Key Parameters: Add comments to important parameters to improve readability</li> <li>Parameter Tuning: For new tasks, start with existing configurations and gradually adjust parameters</li> <li>Version Control: Configuration files for important experiments should be version controlled to reproduce results</li> </ol>"},{"location":"contributing/","title":"\ud83d\udee0\ufe0f Contributing Guidelines","text":"<p>We welcome contributions to OpenHAIV \ud83e\udd17</p> <p>\ud83d\udc47 If you're interested in improving the project, please follow these guidelines</p>"},{"location":"contributing/#code-style","title":"\ud83e\uddf9 Code Style","text":"<p>This project uses pre-commit to automatically enforce code style and quality before each commit. Please install pre-commit and run:</p> Bash<pre><code>pip install pre-commit\npre-commit install\n</code></pre> <p>The main checks include:</p> <ul> <li>flake8: PEP8 code style checking</li> <li>yapf: automatic Python code formatting</li> <li>codespell: spell checking</li> <li>docformatter: automatic docstring formatting</li> <li>trailing-whitespace, end-of-file-fixer, mixed-line-ending and other basic formatting fixes</li> </ul> <p>See the <code>.pre-commit-config.yaml</code> file for detailed configuration. All these checks and fixes will be run automatically before every commit.</p>"},{"location":"contributing/#code-of-conduct","title":"\ud83d\udcdc Code of Conduct","text":"<p>Please note that all contributors are expected to follow our Code of Conduct to foster a welcoming and inclusive community.</p>"},{"location":"contributing/#reporting-issues","title":"\ud83d\udc1b Reporting Issues","text":"<p>Warning</p> <p>Please follow the guidelines for reporting issues:</p> <ol> <li>Check existing issues first to avoid duplicates</li> <li>Use the issue template when available</li> <li>Be specific about the problem:<ul> <li>Include steps to reproduce</li> <li>Provide environment details (OS, Python version, dependencies)</li> <li>Add screenshots if applicable</li> <li>Describe expected vs. actual behavior</li> </ul> </li> </ol>"},{"location":"contributing/#submitting-pull-requests","title":"\ud83d\udca1 Submitting Pull Requests","text":"<p>Warning</p> <p>Make sure to follow the guidelines for submitting a pull request:</p> <ol> <li>Create an issue first to discuss major changes</li> <li>Fork the repository and create a branch from <code>main</code></li> <li>Follow the coding style used throughout the project:<ul> <li>Adhere to PEP 8 guidelines</li> <li>Use meaningful variable/function names</li> <li>Add docstrings for new functions/classes</li> </ul> </li> <li>Write tests for new features</li> <li>Ensure all tests pass before submitting</li> <li>Update documentation reflecting your changes</li> <li>Make atomic commits with clear messages</li> </ol>"},{"location":"methods/","title":"\ud83d\udcda Supported Methods","text":""},{"location":"methods/#class-incremental-learning","title":"\ud83c\udf31 Class-Incremental Learning","text":""},{"location":"methods/#cnn-based-methods","title":"CNN-based methods","text":"Method Paper Venue <code>Joint</code> update models using all the data from all classes <code>Finetune</code> baseline method which simply updates model using current data <code>LwF</code> Learning without Forgetting ECCV 2016 <code>EWC</code> Overcoming catastrophic forgetting in neural networks PNAS 2017 <code>iCaRL</code> Incremental Classifier and Representation Learning CVPR 2017 <code>BiC</code> Large Scale Incremental Learning CVPR 2019 <code>WA</code> Maintaining Discrimination and Fairness in Class Incremental Learning CVPR 2020 <code>DER</code> Dynamically Expandable Representation for Class Incremental Learning CVPR 2021 <code>Coil</code> Co-Transport for Class-Incremental Learning ACM MM 2021 <code>GEM</code> Gradient Episodic Memory for Continual Learning NIPS 2017 <code>SSRE</code> Self-Sustaining Representation Expansion for Non-Exemplar Class-Incremental Learning CVPR 2022 <code>FOSTER</code> Feature Boosting and Compression for Class-incremental Learning ECCV 2022 <code>FeTrIL</code> Feature Translation for Exemplar-Free Class-Incremental Learning WACV 2023 <code>MEMO</code> Memory-Efficient Class-Incremental Learning ICLR 2023"},{"location":"methods/#vit-based-methods","title":"ViT-based methods","text":"Method Paper Venue <code>Joint</code> update models using all the data from all classes <code>L2P</code> Learning to Prompt for Continual Learning CVPR 2022 <code>DualPrompt</code> DualPrompt: Complementary Prompting for Rehearsal-free Continual Learning ECCV 2022 <code>CODA-Prompt</code> CODA-Prompt: COntinual Decomposed Attention-based Prompting for Rehearsal-Free Continual Learning CVPR 2023 <code>S-Prompt</code> S-Prompts Learning with Pre-trained Transformers: An Occam's Razor for Domain Incremental Learning NeurIPS 2022"},{"location":"methods/#few-shot-class-incremental-learning","title":"Few-shot class-incremental learning","text":"Method Paper Venue <code>Alice</code> Few-Shot Class-Incremental Learning from an Open-Set Perspective ECCV 2022 <code>FACT</code> Forward Compatible Few-Shot Class-Incremental Learning CVPR 2022 <code>SAVC</code> Semantic-Aware Virtual Contrastive Constraint for Few-Shot Class-Incremental Learning CVPR 2023"},{"location":"methods/#out-of-distribution-detection","title":"\ud83d\udea8 Out-of-Distribution Detection","text":""},{"location":"methods/#cnn-based-methods_1","title":"CNN-based Methods","text":"Method Paper Venue <code>MSP</code> A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks ICLR 2017 <code>ODIN</code> Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks ICLR 2018 <code>MDS</code> A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks NeurIPS 2018 <code>MLS</code> Scaling Out-of-Distribution Detection for Real-World Settings ICML 2022 <code>ViM</code> Out-Of-Distribution with Virtual-logit Matching CVPR 2022 <code>FDBD</code> Fast Decision Boundary based Out-of-Distribution Detector ICML 2024 <code>VOS</code> Learning What You Don't Know by Virtual Outlier Synthesis ICLR 2022 <code>LogitNorm</code> Mitigating Neural Network Overconfidence with Logit Normalization ICML 2022 <code>DML</code> Decoupling MaxLogit for Out-of-Distribution Detection CVPR 2023"},{"location":"methods/#clip-based-methods","title":"CLIP-based Methods","text":"Method Paper Venue <code>MCM</code> Delving into Out-of-Distribution Detection with Vision-Language Representations NeurIPS 2022 <code>GLMCM</code> Global and Local Maximum Concept Matching for Zero-Shot Out-of-Distribution Detection IJCV 2025 <code>CoOp</code> Learning to Prompt for Vision-Language Models IJCV 2022 <code>LoCoOp</code> Few-Shot Out-of-Distribution Detection via Prompt Learning NeurIPS 2023 <code>SCT</code> Self-Calibrated Tuning of Vision-Language Models for Out-of-Distribution Detection NeurIPS 2024 <code>DPM</code> Vision-Language Dual-Pattern Matching for Out-of-Distribution Detection ECCV 2024"},{"location":"methods/#novel-class-discovery","title":"\ud83d\udd0d Novel Class Discovery","text":"<p>TBD</p>"},{"location":"methods/#data-augmentation","title":"\ud83e\uddec Data Augmentation","text":"<p>TBD</p>"},{"location":"ncdia/","title":"\ud83e\udde9 NCDIA","text":""},{"location":"ncdia/#ncdiaalgorithms","title":"ncdia.algorithms","text":"<p>NCDIA stands for Novel-Category Discovery and Incremental Adaptation, which includes incremental learning algorithms, novel class discovery algorithms, and out-of-distribution detection algorithms, as well as supervised learning algorithms.</p>"},{"location":"ncdia/#ncdiaalgorithmsbasepy","title":"ncdia.algorithms.base.py","text":""},{"location":"ncdia/#basealg","title":"BaseAlg","text":"<p>Basic algorithm class to define the interface of an algorithm.</p> <ul> <li> <p>__init__(self, trainer)</p> <p>The constructor method that initializes an instance of BaseAlg.</p> <p>Parameters:</p> <ul> <li>trainer (object): Trainer object.</li> </ul> </li> <li> <p>train_step(self, trainer, data, label, *args, kwargs)**</p> <p>Training step.</p> <p>Parameters:</p> <ul> <li>trainer (object): Trainer object.</li> <li>data (torch.Tensor): Input data.</li> <li>label (torch.Tensor): Label data.</li> <li>args (tuple): Additional arguments.</li> <li>kwargs (dict): Additional keyword arguments.</li> </ul> <p>Returns:</p> <ul> <li>results (dict): Training results. Contains the following keys:<ul> <li>\"loss\": Loss value.</li> <li>\"acc\": Accuracy value.</li> <li>other \"key:value\" pairs.</li> </ul> </li> </ul> </li> <li> <p>val_step(self, trainer, data, label, *args, kwargs)**</p> <p>Validation step.</p> <p>Parameters:</p> <ul> <li>trainer (object): Trainer object.</li> <li>data (torch.Tensor): Input data.</li> <li>label (torch.Tensor): Label data.</li> <li>args (tuple): Additional arguments.</li> <li>kwargs (dict): Additional keyword arguments.</li> </ul> <p>Returns:</p> <ul> <li>results (dict): Validation results. Contains the following keys:<ul> <li>\"loss\": Loss value.</li> <li>\"acc\": Accuracy value.</li> <li>other \"key:value\" pairs.</li> </ul> </li> </ul> </li> <li> <p>test_step(self, trainer, data, label, *args, kwargs)**</p> <p>Test step.</p> <p>Parameters:</p> <ul> <li>trainer (object): Trainer object.</li> <li>data (torch.Tensor): Input data.</li> <li>label (torch.Tensor): Label data.</li> <li>args (tuple): Additional arguments.</li> <li>kwargs (dict): Additional keyword arguments.</li> </ul> <p>Returns:</p> <ul> <li>results (dict): Test results. Contains the following keys:<ul> <li>\"loss\": Loss value.</li> <li>\"acc\": Accuracy value.</li> <li>other \"key:value\" pairs.</li> </ul> </li> </ul> </li> </ul>"},{"location":"ncdia/#ncdiaalgorithmsincremental","title":"ncdia.algorithms.incremental","text":"<p>Include implementation of Class Incremental Learning (CIL) and Few-shot Class Incremental Learning (FSCIL) algorithm.</p> <ul> <li>CIL<ul> <li>Finetune</li> <li>LwF</li> <li>EwC</li> <li>iCarL</li> <li>IL2A</li> <li>WA </li> </ul> </li> <li>FSCIL<ul> <li>ALICE</li> <li>FACT</li> <li>SAVC</li> </ul> </li> </ul>"},{"location":"ncdia/#ncdiaalgorithmsncdautoncdpy","title":"ncdia.algorithms.ncd.autoncd.py","text":"<p>Modules related to novel class discovery.</p>"},{"location":"ncdia/#autoncd","title":"AutoNCD","text":"<p>Class for evaluating with OOD metrics and relabeling the OOD dataset for the next session.</p> <ul> <li> <p>__init__(self, model, train_loader, test_loader, device=None, verbose=False)</p> <p>The constructor method that initializes an instance of AutoNCD. </p> <p>Parameters:</p> <ul> <li>model (nn.Module): model to be evaluated.</li> <li>train_loader (DataLoader): train dataloader.</li> <li>test_loader (DataLoader): test dataloader.</li> <li>device (torch.device, optional): device to run the evaluation. Default to None.</li> <li>verbose (bool, optional): print the progress bar. Default to False. </li> </ul> </li> <li> <p>inference(self, dataloader, split='train')</p> <p>Inference the model on the dataloader and return relevant information. If split is 'train', return the prototype of the training data. </p> <p>Parameters:</p> <ul> <li>dataloader (DataLoader): dataloader for evaluation.</li> <li>split (str, optional): train or test. Defaults to 'train'.</li> </ul> <p>Returns:</p> <p>If split is 'train':</p> <ul> <li> <p>features (torch.Tensor): feature vectors, (N, D).</p> </li> <li> <p>logits (torch.Tensor): logit vectors, (N, C).</p> </li> <li> <p>prototype_cls (torch.Tensor): prototype vectors, (C, D).</p> </li> </ul> <p>If split is 'test':</p> <ul> <li> <p>imgpaths (list): image paths (list).</p> </li> <li> <p>features (torch.Tensor): feature vectors, (N, D).</p> </li> <li> <p>logits (torch.Tensor): logit vectors, (N, C).</p> </li> <li> <p>preds (torch.Tensor): prediction labels, (N,).</p> </li> <li> <p>labels (torch.Tensor): ground truth labels, (N,).</p> </li> </ul> </li> <li> <p>relabel(self, ood_loader, metrics=[], tpr_th=0.95, prec_th=None)</p> <p>Relabel the OOD dataset for the next session.</p> <p>Parameters:</p> <ul> <li>ood_loader (DataLoader): OOD dataloader for relabeling.</li> <li>metrics (list, optional): metrics to evaluate the OOD dataset. Defaults to [].</li> <li>tpr_th (float, optional): True positive rate threshold. Defaults to 0.95.</li> <li>prec_th (float, optional): Precision threshold. Defaults to None.</li> </ul> <p>Returns:</p> <ul> <li>ood_loader (DataLoader): relabeled OOD dataloader.</li> </ul> </li> <li> <p>_split_cluster_label(self, y_label, y_pred, ood_class)</p> <p>Calculate clustering accuracy. Require scikit-learn installed. First compute linear assignment on all data, then look at how good the accuracy is on subsets.</p> <p>Parameters:</p> <ul> <li>y_label (numpy.array): true labels, (n_samples,)</li> <li>y_pred (numpy.array): predicted labels (n_samples,)</li> <li>ood_class: out-of-distribution class labels</li> </ul> <p>Returns:</p> <ul> <li>cluster_label: cluster label</li> </ul> </li> <li> <p>search_discrete_point(self, novel_feat, novel_target)</p> </li> </ul> <p>TODO</p>"},{"location":"ncdia/#ncdiaalgorithmsoodautooodpy","title":"ncdia.algorithms.ood.autoood.py","text":""},{"location":"ncdia/#autoood","title":"AutoOOD","text":"<p>Class for evaluating OOD detection methods.</p> <ul> <li> <p>eval(prototype_cls, fc_weight, train_feats, train_logits, id_feats, id_logits, id_labels, ood_feats, ood_logits, ood_labels, metrics=[], tpr_th=0.95, prec_th=None, id_attrs=None, ood_attrs=None, prototype_att=None)</p> <p>Evaluate the OOD detection methods and return OOD scores.</p> <p>Parameters:</p> <ul> <li>prototype_cls (np.ndarray): prototype of training data</li> <li>fc_weight (np.ndarray): weight of the last layer</li> <li>train_feats (np.ndarray): feature of training data</li> <li>train_logits (np.ndarray): logits of training data</li> <li>id_feats (np.ndarray): feature of ID data</li> <li>id_logits (np.ndarray): logits of ID data</li> <li>id_labels (np.ndarray): labels of ID data</li> <li>ood_feats (np.ndarray): feature of OOD data</li> <li>ood_logits (np.ndarray): logits of OOD data</li> <li>ood_labels (np.ndarray): labels of OOD data</li> <li>metrics (list, optional): list of OOD detection methods to evaluate. Defaults to [].</li> <li>tpr_th (float, optional): True positive rate threshold. Defaults to 0.95.</li> <li>prec_th (float, optional): Precision threshold. Defaults to None.</li> </ul> <p>Returns:</p> <ul> <li>ood_scores (dict): OOD scores, keys are the names of the OOD detection methods, values are the OOD scores and search threshold. Each value is a tuple containing the following:<ul> <li>ood metrics (tuple):<ul> <li>fpr (float): false positive rate</li> <li>auroc (float): area under the ROC curve</li> <li>aupr_in (float): area under the precision-recall curve for in-distribution samples</li> <li>aupr_out (float): area under the precision-recall curve for out-of-distribution samples</li> </ul> </li> <li>search threshold (tuple): threshold for OOD detection if prec_th is not None<ul> <li>best_th (float): best threshold for OOD detection</li> <li>conf (torch.Tensor): confidence scores</li> <li>label (torch.Tensor): label array</li> <li>precisions (float): precision when precisions &gt;= prec_th</li> <li>recalls (float): recall when precisions &gt;= prec_th</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>inference(metrics, logits, feat, train_logits, train_feat, fc_weight, prototype, logits_att=None, prototype_att=None)</p> <p>Inferencec method for OOD detection</p> <p>Parameters:</p> <ul> <li>metrics (list): the ood metrics used for inference.</li> <li>logits (np.ndarray): logits of inference data.</li> <li>feat (np.ndarray): features of inference data.</li> <li>train_logits (np.ndarray): logits of training data.</li> <li>train_feat (np.ndarray): features of training data.</li> <li>fc_weight (np.ndarray): weight of the last layer.</li> <li>prototype (np.ndarray): prototypes of training data.</li> <li>logits_att (np.ndarray, optional): logits of attribute.</li> <li>prototype_att (np.ndarray, optional): prototypes of attribute. </li> </ul> <p>Returns:</p> <ul> <li>conf (dict): contains the confidence using different metrics, conf[metric] (torch.Tensor) is the confidence using specific metric.</li> </ul> </li> </ul>"},{"location":"ncdia/#ncdiaalgorithmsoodmethodspy","title":"ncdia.algorithms.ood.methods.py","text":"<ul> <li> <p>msp(id_gt, id_logits, ood_gt, ood_logits, tpr_th=0.95, prec_th=None)</p> <p>Maximum Softmax Probability (MSP) method for OOD detection.</p> <p>A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks.</p> <p>Parameters:</p> <ul> <li>id_gt (torch.Tensor): ID ground truth labels. Shape (N,).</li> <li>id_logits (torch.Tensor): ID logits. Shape (N, C).</li> <li>ood_gt (torch.Tensor): OOD ground truth labels. Shape (M,).</li> <li>ood_logits (torch.Tensor): OOD logits. Shape (M, C).</li> <li>tpr_th (float): True positive rate threshold to compute     false positive rate. Default is 0.95.</li> <li>prec_th (float): Precision threshold for searching - threshold.     If None, not searching for threshold. Default is None.</li> </ul> <p>Returns:</p> <ul> <li>conf (np.ndarray): Confidence scores. Shape (N + M,).</li> <li>label (np.ndarray): Label array. Shape (N + M,).</li> <li>fpr (float): False positive rate.</li> <li>auroc (float): Area under the ROC curve.</li> <li>aupr_in (float): Area under the precision-recall curve      for in-distribution samples.</li> <li>aupr_out (float): Area under the precision-recall curve     for out-of-distribution</li> <li>best_th (float): Threshold for OOD detection. If prec_th is None, None.</li> <li>prec (float): Precision at the threshold. If prec_th is None, None.</li> <li>recall (float): Recall at the threshold. If prec_th is None, None.</li> </ul> </li> <li> <p>mcm(id_gt, id_logits, ood_gt, ood_logits, T=2, tpr_th=0.95, prec_th=None)</p> <p>Maximum Concept Matching (MCM) method for OOD detection.</p> <p>Delving into Out-of-Distribution Detection with Vision-Language Representations</p> <p>Parameters:</p> <ul> <li>id_gt (torch.Tensor): ID ground truth labels. Shape (N,).</li> <li>id_logits (torch.Tensor): ID logits. Shape (N, C).</li> <li>ood_gt (torch.Tensor): OOD ground truth labels. Shape (M,).</li> <li>ood_logits (torch.Tensor): OOD logits. Shape (M, C).</li> <li>T (int): Temperature for softmax.</li> <li>tpr_th (float): True positive rate threshold to compute     false positive rate. Default is 0.95.</li> <li>prec_th (float): Precision threshold for searching threshold.     If None, not searching for threshold. Default is None.</li> </ul> <p>Returns:</p> <ul> <li>fpr (float): False positive rate.</li> <li>auroc (float): Area under the ROC curve.</li> <li>aupr_in (float): Area under the precision-recall curve      for in-distribution samples.</li> <li>aupr_out (float): Area under the precision-recall curve     for out-of-distribution.</li> </ul> </li> <li> <p>max_logit(id_gt, id_logits, ood_gt, ood_logits, tpr_th=0.95, prec_th=None)</p> <p>Maximum Logit (MaxLogit) method for OOD detection.</p> <p>Scaling Out-of-Distribution Detection for Real-World Settings</p> <p>Parameters:</p> <ul> <li>id_gt (torch.Tensor): ID ground truth labels. Shape (N,).</li> <li>id_logits (torch.Tensor): ID logits. Shape (N, C).</li> <li>ood_gt (torch.Tensor): OOD ground truth labels. Shape (M,).</li> <li>ood_logits (torch.Tensor): OOD logits. Shape (M, C).</li> <li>tpr_th (float): True positive rate threshold to compute     false positive rate. Default is 0.95.</li> <li>prec_th (float): Precision threshold for searching threshold.     If None, not searching for threshold. Default is None.</li> </ul> <p>Returns:</p> <ul> <li>fpr (float): False positive rate.</li> <li>auroc (float): Area under the ROC curve.</li> <li>aupr_in (float): Area under the precision-recall curve      for in-distribution samples.</li> <li>aupr_out (float): Area under the precision-recall curve     for out-of-distribution</li> </ul> </li> <li> <p>energy(id_gt, id_logits, ood_gt, ood_logits, tpr_th=0.95, prec_th=None)</p> <p>Energy-based method for OOD detection.</p> <p>Energy-based Out-of-distribution Detection</p> <p>Parameters:</p> <ul> <li>id_gt (torch.Tensor): ID ground truth labels. Shape (N,).</li> <li>id_logits (torch.Tensor): ID logits. Shape (N, C).</li> <li>ood_gt (torch.Tensor): OOD ground truth labels. Shape (M,).</li> <li>ood_logits (torch.Tensor): OOD logits. Shape (M, C).</li> <li>tpr_th (float): True positive rate threshold to compute     false positive rate. Default is 0.95.</li> <li>prec_th (float): Precision threshold for searching threshold.     If None, not searching for threshold. Default is None.</li> </ul> <p>Returns:</p> <ul> <li>fpr (float): False positive rate.</li> <li>auroc (float): Area under the ROC curve.</li> <li>aupr_in (float): Area under the precision-recall curve      for in-distribution samples.</li> <li>aupr_out (float): Area under the precision-recall curve     for out-of-distribution</li> </ul> </li> <li> <p>vim(id_gt, id_logits, id_feat, ood_gt, ood_logits, ood_feat, train_logits, train_feat, tpr_th=0.95, prec_th=None)</p> <p>Virtual-Logit Matching (ViM) method for OOD detection.</p> <p>ViM: Out-of-Distribution With Virtual-Logit Matching</p> <p>Parameters:</p> <ul> <li>id_gt (torch.Tensor): ID ground truth labels. Shape (N,).</li> <li>id_logits (torch.Tensor): ID logits. Shape (N, C).</li> <li>id_feat (torch.Tensor): ID features. Shape (N, D).</li> <li>ood_gt (torch.Tensor): OOD ground truth labels. Shape (M,).</li> <li>ood_logits (torch.Tensor): OOD logits. Shape (M, C).</li> <li>ood_feat (torch.Tensor): OOD features. Shape (M, D).</li> <li>train_logits (torch.Tensor): Training logits. Shape (K, C).</li> <li>train_feat (torch.Tensor): Training features. Shape (K, D).</li> <li>tpr_th (float): True positive rate threshold to compute     false positive rate. Default is 0.95.</li> <li>prec_th (float): Precision threshold for searching threshold.     If None, not searching for threshold. Default is None.</li> </ul> <p>Returns:</p> <ul> <li>fpr (float): False positive rate.</li> <li>auroc (float): Area under the ROC curve.</li> <li>aupr_in (float): Area under the precision-recall curve      for in-distribution samples.</li> <li>aupr_out (float): Area under the precision-recall curve     for out-of-distribution</li> </ul> </li> <li> <p>dml(id_gt, id_feat, ood_gt, ood_feat, fc_weight, tpr_th=0.95, prec_th=None)</p> <p>Decoupled MaxLogit (DML) method for OOD detection.</p> <p>Decoupling MaxLogit for Out-of-Distribution Detection</p> <p>Parameters:</p> <ul> <li>id_gt (torch.Tensor): ID ground truth labels. Shape (N,).</li> <li>id_feat (torch.Tensor): ID features. Shape (N, D).</li> <li>ood_gt (torch.Tensor): OOD ground truth labels. Shape (M,).</li> <li>ood_feat (torch.Tensor): OOD features. Shape (M, D).</li> <li>fc_weight (torch.Tensor): FC layer weight. Shape (C, D).</li> <li>tpr_th (float): True positive rate threshold to compute     false positive rate. Default is 0.95.</li> <li>prec_th (float): Precision threshold for searching threshold.     If None, not searching for threshold. Default is None.</li> </ul> <p>Returns:</p> <ul> <li>fpr (float): False positive rate.</li> <li>auroc (float): Area under the ROC curve.</li> <li>aupr_in (float): Area under the precision-recall curve      for in-distribution samples.</li> <li>aupr_out (float): Area under the precision-recall curve     for out-of-distribution</li> </ul> </li> <li> <p>dmlp(id_gt, id_logits, id_feat, ood_gt, ood_logits, ood_feat, fc_weight, prototype,tpr_th=0.95, prec_th=None)</p> <p>Decoupled MaxLogit+ (DML+) method for OOD detection.</p> <p>Decoupling MaxLogit for Out-of-Distribution Detection</p> <p>Parameters:</p> <ul> <li>id_gt (torch.Tensor): ID ground truth labels. Shape (N,).</li> <li>id_logits (torch.Tensor): ID logits. Shape (N, C).</li> <li>id_feat (torch.Tensor): ID features. Shape (N, D).</li> <li>ood_gt (torch.Tensor): OOD ground truth labels. Shape (M,).</li> <li>ood_logits (torch.Tensor): OOD logits. Shape (M, C).</li> <li>ood_feat (torch.Tensor): OOD features. Shape (M, D).</li> <li>fc_weight (torch.Tensor): FC layer weight. Shape (D, C).</li> <li>prototype (torch.Tensor): Prototype. Shape (D, C).</li> <li>tpr_th (float): True positive rate threshold to compute     false positive rate. Default is 0.95.</li> <li>prec_th (float): Precision threshold for searching threshold.     If None, not searching for threshold. Default is None.</li> </ul> <p>Returns:</p> <ul> <li>fpr (float): False positive rate.</li> <li>auroc (float): Area under the ROC curve.</li> <li>aupr_in (float): Area under the precision-recall curve      for in-distribution samples.</li> <li>aupr_out (float): Area under the precision-recall curve     for out-of-distribution</li> </ul> </li> <li> <p>prot(id_gt, id_logits, ood_gt, ood_logits, prototypes: list, tpr_th=0.95, prec_th=None)</p> <p>Prototype-based (Prot) method for OOD detection.</p> <p>Parameters:</p> <ul> <li>id_gt (torch.Tensor): ID ground truth labels, shape (N,).</li> <li>id_logits (list of torch.Tensor): ID logits, containing shape (N, C).</li> <li>ood_gt (torch.Tensor): OOD ground truth labels, shape (M,).</li> <li>ood_logits (list of torch.Tensor): OOD logits, containing shape (M, C).</li> <li>prototypes (list of torch.Tensor): Prototypes, containing shape (D, C).</li> <li>tpr_th (float): True positive rate threshold to compute      false positive rate.</li> <li>prec_th (float): Precision threshold for searching threshold.     If None, not searching for threshold. Default is</li> </ul> <p>Returns:</p> <ul> <li>fpr (float): False positive rate.</li> <li>auroc (float): Area under the ROC curve.</li> <li>aupr_in (float): Area under the precision-recall curve      for in-distribution samples.</li> <li>aupr_out (float): Area under the precision-recall curve     for out-of-distribution</li> </ul> </li> </ul>"},{"location":"ncdia/#ncdiaalgorithmsoodinferencepy","title":"ncdia.algorithms.ood.inference.py","text":"<p>The inference version of implemented ood methods in ncdia.algorithms.ood.methods.py</p>"},{"location":"ncdia/#ncdiaalgorithmsoodmetricspy","title":"ncdia.algorithms.ood.metrics.py","text":"<ul> <li> <p>ood_metrics(conf, label, tpr_th=0.95)</p> <p>Compute OOD metrics.</p> <p>Parameters:</p> <ul> <li>conf (np.ndarray): Confidence scores. Shape (N,).</li> <li>label (np.ndarray): Label array. Shape (N,). Containing:     -1: OOD samples.     int &gt;= 0: ID samples with class labels</li> <li>tpr_th (float): True positive rate threshold to compute      false positive rate.</li> </ul> <p>Returns:</p> <ul> <li>fpr (float): False positive rate.</li> <li>auroc (float): Area under the ROC curve.</li> <li>aupr_in (float): Area under the precision-recall curve      for in-distribution samples.</li> <li>aupr_out (float): Area under the precision-recall curve     for out-of-distribution samples.</li> </ul> </li> <li> <p>search_threshold(conf, label, prec_th)</p> <p>Search for the threshold for OOD detection.</p> <p>Parameters:</p> <ul> <li>conf (np.ndarray): Confidence scores. Shape (N,).</li> <li>label (np.ndarray): Label array. Shape (N,). Containing:     -1: OOD samples.     int &gt;= 0: ID samples with class labels</li> <li>prec_th (float): Precision threshold.</li> </ul> <p>Returns:</p> <ul> <li>best_th (float): Threshold for OOD detection.</li> <li>prec (float): Precision at the threshold.</li> <li>recall (float): Recall at the threshold.</li> </ul> </li> </ul>"},{"location":"ncdia/#ncdiaalgorithmssupervisedstandardpy","title":"ncdia.algorithms.supervised.standard.py","text":"<p>Modules related to supervised learning</p>"},{"location":"ncdia/#standardsl","title":"StandardSL","text":"<p>Class inherits from BaseAlg. Standard supervised learning algorithm</p> <ul> <li> <p>train_step(self, trainer, data, label, *args, **kwargs)</p> <p>Training step for standard supervised learning.</p> <p>Parameters:</p> <ul> <li>trainer (object): Trainer object.</li> <li>data (torch.Tensor): Input data.</li> <li>label (torch.Tensor): Label data.</li> <li>args (tuple): Additional arguments.</li> <li>kwargs (dict): Additional keyword arguments.</li> </ul> <p>Returns:</p> <ul> <li> <p>results (dict): Training results. Contains the following keys:</p> <ul> <li>\"loss\": Loss value.</li> <li>\"acc\": Accuracy value.</li> </ul> </li> </ul> </li> <li> <p>val_step(self, trainer, data, label, *args, **kwargs)</p> <p>Validation step for standard supervised learning.</p> <p>Parameters:</p> <ul> <li>trainer (object): Trainer object.</li> <li>data (torch.Tensor): Input data.</li> <li>label (torch.Tensor): Label data.</li> <li>args (tuple): Additional arguments.</li> <li>kwargs (dict): Additional keyword arguments.</li> </ul> <p>Returns:</p> <ul> <li> <p>results (dict): Validation results. Contains the following:</p> <ul> <li>\"loss\": Loss value.</li> <li>\"acc\": Accuracy value.</li> </ul> </li> </ul> </li> <li> <p>test_step(self, trainer, data, label, *args, **kwargs)</p> <p>Test step for standard supervised learning.</p> <p>Parameters:</p> <ul> <li>trainer (object): Trainer object.</li> <li>data (torch.Tensor): Input data.</li> <li>label (torch.Tensor): Label data.</li> <li>args (tuple): Additional arguments.</li> <li>kwargs (dict): Additional keyword arguments.</li> </ul> <p>Returns:</p> <ul> <li> <p>results (dict): Test results. Contains the following:</p> <ul> <li>\"loss\": Loss value.</li> <li>\"acc\": Accuracy value.</li> </ul> </li> </ul> </li> </ul>"},{"location":"ncdia/#ncdiadataloader","title":"ncdia.dataloader","text":""},{"location":"ncdia/#ncdiadataloaderbasepy","title":"ncdia.dataloader.base.py","text":"<ul> <li> <p>build_dataloader(kwargs)</p> <p>Build data loader.</p> <p>Parameters:</p> <ul> <li>kwargs (dict): Arguments for DataLoader. Contains the following:<ul> <li>dataset (dict): Dataset configuration.</li> <li>other arguments for DataLoader, such as batch_size, shuffle, etc.</li> </ul> </li> </ul> <p>Returns:</p> <ul> <li>loader (DataLoader): Data loader.</li> </ul> </li> </ul>"},{"location":"ncdia/#ncdiadataloadertoolspy","title":"ncdia.dataloader.tools.py","text":"<p>Implements some of the commonly used dataloaders</p>"},{"location":"ncdia/#ncdiadataloaderdatasets","title":"ncdia.dataloader.datasets","text":"<p>Implements some of the commonly used datasets, including:</p> <ul> <li>CIFAR100</li> <li>CUB200</li> <li>Caltech101</li> <li>Food101</li> <li>ImageNet</li> <li>ImageNetR</li> <li>BM200</li> </ul>"},{"location":"ncdia/#ncdiadataloaderaugmentations","title":"ncdia.dataloader.augmentations","text":"<p>Implements some of the commonly used augmentation methods.</p>"},{"location":"ncdia/#ncdiamodel","title":"ncdia.model","text":""},{"location":"ncdia/#ncdiamodelsmodelspy","title":"ncdia.models.models.py","text":"<ul> <li> <p>get_network(config)</p> <p>load model.</p> <p>Parameters: -trainer(config): model config</p> </li> </ul>"},{"location":"ncdia/#ncdiamodelsnetinc_netpy","title":"ncdia.models.net.inc_net.py","text":""},{"location":"ncdia/#basenet","title":"BaseNet","text":"<p>BaseNet for incremental learning.</p> <ul> <li> <p>__init__(self, network, base_classes, num_classes, att_classes, net_alice, mode)</p> <p>The constructor method that initializes an instance of BaseNet.</p> <p>Parameters:</p> <ul> <li>network (config): The config of the network.</li> <li>base_classes(int): The number of base classes.</li> <li>num_classes(int): The total class number.</li> <li>att_classes(int): The attribute class number.</li> <li>mode(str): classifier mode.</li> </ul> </li> <li> <p>feature_dim(self)</p> <p>The feature dimension of the network.</p> <p>Returns:</p> <ul> <li>out_dim(int) feature dimension of the network.</li> </ul> </li> <li> <p>extractor_vector(self, x)</p> <p>get features of input x.</p> <p>Parameters:</p> <ul> <li>x(tensor): input data.</li> </ul> <p>Returns:</p> <ul> <li>out_features(tensor) features of the input.</li> </ul> </li> <li> <p>forward(self, x)</p> <p>forworad pass of the network.</p> <p>Parameters:</p> <ul> <li>x(tensor): input data.</li> </ul> <p>Returns:</p> <ul> <li>results (dict): forward pass results. Contains the following keys:<ul> <li>\"fmaps\": [x_1, x_2, ..., x_n],</li> <li>\"features\": features</li> <li>\"logits\": logits</li> </ul> </li> </ul> </li> <li> <p>copy(self)</p> <p>copy.</p> <p>Returns:</p> <ul> <li>copy function.</li> </ul> </li> <li> <p>freeze(self)</p> <p>freeze parameters.</p> </li> </ul>"},{"location":"ncdia/#incrementalnet","title":"IncrementalNet","text":"<p>Incremental Network which follows BaseNet.</p> <ul> <li> <p>__init__(self, network, base_classes, num_classes, att_classes, net_alice, mode)</p> <p>The constructor method that initializes an instance of BaseNet.</p> <p>Parameters:</p> <ul> <li>network (config): The config of the network.</li> <li>base_classes(int): The number of base classes.</li> <li>num_classes(int): The total class number.</li> <li>att_classes(int): The attribute class number.</li> <li>mode(str): classifier mode.</li> </ul> </li> <li> <p>update_fc(self, nb_classes)</p> <p>update fc parameter, generate new fc and copy old parameter.</p> <p>Parameters:</p> <ul> <li>network (int): New class number.</li> </ul> <p>Returns:</p> <ul> <li>fc: updated fc layers.</li> </ul> </li> <li> <p>generate_fc(self, in_dim, out_dim)</p> <p>Parameters:</p> <ul> <li>in_dim (int): new fc in dimension.</li> <li>out_dim (int): new fc out dimension.</li> </ul> <p>Returns:</p> <ul> <li>fc: new fc layers.</li> </ul> </li> <li> <p>forward(self, x)</p> <p>forworad pass of the network.</p> <p>Parameters:</p> <ul> <li>x(tensor): input data.</li> </ul> <p>Returns:</p> <ul> <li>results (dict): forward pass results. Contains the following keys:<ul> <li>\"fmaps\": [x_1, x_2, ..., x_n],</li> <li>\"features\": features</li> <li>\"logits\": logits</li> </ul> </li> </ul> </li> <li> <p>weight_align(self, increment)</p> <p>normalize classifer parameters.</p> <p>Parameters:</p> <ul> <li>increment(int): incremental classes.</li> </ul> </li> </ul>"},{"location":"ncdia/#ncdiatrainers","title":"ncdia.trainers","text":""},{"location":"ncdia/#ncdiatrainersbasepy","title":"ncdia.trainers.base.py","text":""},{"location":"ncdia/#basetrainer","title":"BaseTrainer","text":"<p>Basic trainer class for training models.</p> <p>Attributes:</p> <ul> <li>model (nn.Module): Neural network models.</li> <li>train_loader (DataLoader): DataLoader for training.</li> <li>val_loader (DataLoader): DataLoader for validation.</li> <li>test_loader (DataLoader): DataLoader for testing.</li> <li>optimizer (Optimizer): Optimizer.</li> <li>scheduler (lr_scheduler._LRScheduler): Learning rate scheduler.</li> <li>criterion (Callable): Criterion for training.</li> <li>algorithm (object): Algorithm for training.</li> <li>metrics (dict): Metrics for evaluation and testing.</li> <li>session (int): Session number.</li> <li>max_epochs (int): Total epochs for training.</li> <li>max_train_iters (int): Iterations on one epoch for training.</li> <li>max_val_iters (int): Iterations on one epoch for validation.</li> <li>max_test_iters (int): Iterations on one epoch for testing.</li> <li>epoch (int): Current training epoch.</li> <li>iter (int): Current iteration or index of the current batch.</li> <li>cfg (Configs): Configuration for trainer.</li> <li>hooks (List[Hook]): List of registered hooks.</li> <li>logger (Logger): Logger for logging information.</li> <li>device (torch.device): Device to use.</li> <li>work_dir (str): Working directory to save logs and checkpoints.</li> <li>exp_name (str): Experiment name.</li> <li>load_from (str): Checkpoint file path to load.</li> </ul> <p>Methods:</p> <ul> <li> <p> __init__(self, cfg, session, model, train_loader, val_loader, test_loader, default_hooks, custom_hooks, load_from, exp_name, work_dir)</p> <p>The constructor method that initializes an instance of BaseTrainer. </p> <p>Parameters:</p> <ul> <li>cfg (dict, optional): Configuration for trainer, Contains:<ul> <li>'trainer' (dict):<ul> <li>'type' (str): Type of trainer.</li> </ul> </li> <li>'algorithm' (dict):<ul> <li>'type' (str): Type of algorithm.</li> </ul> </li> <li>'criterion' (dict):<ul> <li>'type' (str): Type of criterion for training.</li> </ul> </li> <li>'optimizer':<ul> <li>'type' (str): Name of optimizer.</li> <li>'param_groups' (dict | None): If provided, directly optimize     param_groups and abandon model.</li> <li>kwargs (dict) for optimizer, such as 'lr', 'weight_decay', etc.</li> </ul> </li> <li>'scheduler':<ul> <li>'type' (str): Name of scheduler.</li> <li>kwargs (dict) for scheduler, such as 'step_size', 'gamma', etc.</li> </ul> </li> <li>'device' (str | torch.device | None): Device to use.     If None, use 'cuda' if available.</li> <li>'trainloader':<ul> <li>'dataset': <ul> <li>'type' (str): Type of dataset.</li> <li>kwargs (dict) for dataset, such as 'root', 'split', etc.</li> </ul> </li> <li>kwargs (dict) for DataLoader, such as 'batch_size', 'shuffle', etc.</li> </ul> </li> <li>'valloader':<ul> <li>'dataset': <ul> <li>'type' (str): Type of dataset.</li> <li>kwargs (dict) for dataset, such as 'root', 'split', etc.</li> </ul> </li> <li>kwargs (dict) for DataLoader, such as 'batch_size', 'shuffle', etc.</li> </ul> </li> <li>'testloader':<ul> <li>'dataset':<ul> <li>'type' (str): Type of dataset.</li> <li>kwargs (dict) for dataset, such as 'root', 'split', etc.</li> </ul> </li> <li>kwargs (dict) for DataLoader, such as 'batch_size', 'shuffle', etc.</li> </ul> </li> <li>'exp_name' (str): Experiment name.</li> <li>'work_dir' (str): Working directory to save logs and checkpoints.</li> </ul> </li> <li>session (int): Session number. If == 0, execute pre-training.     If &gt; 0, execute incremental training.</li> <li>model (nn.Module): Model to be trained.</li> <li>train_loader (DataLoader | dict, optional): DataLoader for training.</li> <li>val_loader (DataLoader | dict, optional): DataLoader for validation.</li> <li>test_loader (DataLoader | dict, optional): DataLoader for testing.</li> <li>default_hooks (dict, optional): Default hooks to be registered.</li> <li>custom_hooks (list, optional): Custom hooks to be registered.</li> <li>load_from (str, optional): Checkpoint file path to load.</li> <li>work_dir (str, optional): Working directory to save logs and checkpoints.</li> </ul> </li> <li> <p> train_step(self, batch, **kwargs)</p> <p>Training step. This method should be implemented in subclasses.</p> <p>Parameters:</p> <ul> <li>batch (dict | tuple | list): A batch of data from the data loader.</li> </ul> <p>Returns:</p> <ul> <li> <p>results (dict): Contains the following:</p> <p>{\"key1\": value1, \"key2\": value2,...}</p> <p>keys denote the description of the value, such as \"loss\", \"acc\", \"ccr\", etc. values are the corresponding values of the keys, can be int, float, str, etc.    </p> </li> </ul> </li> <li> <p> val_step(self, batch, kwargs)**</p> <p>Validation step. This method should be implemented in subclasses.</p> <p>Parameters:</p> <ul> <li>batch (dict | tuple | list): A batch of data from the data loader.</li> </ul> <p>Returns:</p> <ul> <li> <p>results (dict): Contains the following:</p> <p>{\"key1\": value1, \"key2\": value2,...}</p> <p>keys denote the description of the value, such as \"loss\", \"acc\", \"ccr\", etc. values are the corresponding values of the keys, can be int, float, str, etc.    </p> </li> </ul> </li> <li> <p> test_step(self, batch, kwargs)**</p> <p>Test step. This method should be implemented in subclasses.</p> <p>Parameters:</p> <ul> <li>batch (dict | tuple | list): A batch of data from the data loader.</li> </ul> <p>Returns:</p> <ul> <li> <p>results (dict): Contains the following:</p> <p>{\"key1\": value1, \"key2\": value2,...}</p> <p>keys denote the description of the value, such as \"loss\", \"acc\", \"ccr\", etc. values are the corresponding values of the keys, can be int, float, str, etc.   </p> </li> </ul> </li> <li> <p> train(self)</p> <p>Launch the training process.</p> <p>Returns:</p> <ul> <li>model (nn.Module): Trained model.</li> </ul> </li> <li> <p> val(self)</p> </li> </ul> <p>Validation process.</p> <ul> <li> test(self)</li> </ul> <p>Test process.</p> <ul> <li> <p> load_ckpt(self, fpath, device='cpu')</p> <p>Load checkpoint from file.</p> <p>Parameters:</p> <ul> <li>fpath (str): Checkpoint file path.</li> <li>device (str): Device to load checkpoint. Defaults to 'cpu'.</li> </ul> <p>Returns:</p> <ul> <li>model (nn.Module): Loaded model.</li> </ul> </li> <li> <p> save_ckpt(self, fpath)</p> <p>Save checkpoint to file.</p> <p>Parameters:</p> <ul> <li>fpath (str): Checkpoint file path.</li> </ul> </li> <li> <p> call_hook(self, fn_name: str, kwargs)**</p> <p>Call all hooks with the specified function name.</p> <p>Parameters:</p> <ul> <li> <p>fn_name (str): Function name to be called, such as:</p> <ul> <li>'before_train_epoch'</li> <li>'after_train_epoch'</li> <li>'before_train_iter'</li> <li>'after_train_iter'</li> <li>'before_val_epoch'</li> <li>'after_val_epoch'</li> <li>'before_val_iter'</li> <li>'after_val_iter'</li> </ul> </li> <li> <p>kwargs (dict): Arguments for the function.</p> </li> </ul> </li> <li> <p>register_hook(self, hook, priority=None)</p> <p>Register a hook into the hook list.</p> <p>The hook will be inserted into a priority queue, with the specified priority (See :class:<code>Priority</code> for details of priorities). For hooks with the same priority, they will be triggered in the same order as they are registered. Priority of hook will be decided with the following priority:</p> <ul> <li><code>priority</code> argument. If <code>priority</code> is given, it will be priority of hook.</li> <li>If <code>hook</code> argument is a dict and <code>priority</code> in it, the priority will be the value of <code>hook['priority']</code>.</li> <li>If <code>hook</code> argument is a dict but <code>priority</code> not in it or <code>hook</code> is an instance of <code>hook</code>, the priority will be <code>hook.priority</code>.</li> </ul> <p>Parameters:</p> <ul> <li>hook (:obj:<code>Hook</code> or dict): The hook to be registered. priority (int or str or :obj:<code>Priority</code>, optional): Hook priority. Lower value means higher priority.</li> </ul> </li> <li> <p> register_default_hooks(self, hooks=None)</p> <p>Register default hooks into hook list.</p> <p><code>hooks</code> will be registered into runner to execute some default actions like updating model parameters or saving checkpoints.</p> <p>Default hooks and their priorities:</p> Hooks Priority RuntimeInfoHook VERY_HIGH (10) IterTimerHook NORMAL (50) DistSamplerSeedHook NORMAL (50) LoggerHook BELOW_NORMAL (60) ParamSchedulerHook LOW (70) CheckpointHook VERY_LOW (90) <p>If <code>hooks</code> is None, above hooks will be registered by default:</p> Text Only<pre><code>default_hooks = dict(\n    logger=dict(type='LoggerHook'),\n    model=dict(type='ModelHook'),\n    alg=dict(type='AlgHook'),\n    optimizer = dict(type='OptimizerHook'),\n    scheduler = dict(type='SchedulerHook'),\n    metric = dict(type='MetricHook'),\n)\n</code></pre> <p>If not None, <code>hooks</code> will be merged into <code>default_hooks</code>. If there are None value in default_hooks, the corresponding item will be popped from <code>default_hooks</code>:</p> Text Only<pre><code>hooks = dict(timer=None)\n</code></pre> <p>The final registered default hooks will be :obj:<code>RuntimeInfoHook</code>, :obj:<code>DistSamplerSeedHook</code>, :obj:<code>LoggerHook</code>, :obj:<code>ParamSchedulerHook</code> and :obj:<code>CheckpointHook</code>.</p> <p>Parameters:</p> <ul> <li>hooks (dict[str, Hook or dict]): Default hooks or configs to be registered.</li> </ul> </li> <li> <p> register_custom_hooks(self, hooks)</p> <p>Register custom hooks into hook list.</p> <p>Parameters:</p> <p>hooks (list[Hook | dict]): List of hooks or configs to be registered.</p> </li> <li> <p> register_hooks(self, default_hooks=None, custom_hooks=None)</p> <p>Register default hooks and custom hooks into hook list.</p> <p>Parameters:</p> <ul> <li>default_hooks (dict[str, dict] or dict[str, Hook]): Hooks to execute default actions like updating model parameters and saving checkpoints.  Defaults to None.</li> <li>custom_hooks (list[dict] or list[Hook]): Hooks to execute custom actions like visualizing images processed by pipeline. Defaults to None.</li> </ul> </li> <li> <p>get_hooks_info(self)</p> <p>Get registered hooks information.</p> <p>Returns:</p> <ul> <li>info (str): Information of registered hooks.</li> </ul> </li> </ul>"},{"location":"ncdia/#ncdiatrainerspretrainerpy","title":"ncdia.trainers.pretrainer.py","text":""},{"location":"ncdia/#pretrainer","title":"PreTrainer","text":"<p>PreTrainer class for pre-training a model on session 0.</p> <p>Attributes:</p> <ul> <li>max_epochs (int): Total epochs for training.</li> </ul> <p>Methods:</p> <ul> <li>__init__(self, max_epochs=1, **kwargs): The constructor method that initializes an instance of PreTrainer. max_epochs (int): Total epochs for training.</li> <li>train_step(self, batch, **kwargs): Training step.</li> <li>val_step(self, batch, **kwargs): Validation step.</li> <li>test_step(self, batch, **kwargs): Test step.</li> <li> <p>batch_parser(batch) </p> <p>Parse a batch of data.</p> <p>Parameters:</p> <ul> <li>batch (dict | tuple | list): A batch of data.</li> </ul> <p>Returns:</p> <ul> <li>data (torch.Tensor | list): Input data.</li> <li>label (torch.Tensor | list): Label data.</li> <li>attribute (torch.Tensor | list): Attribute data.</li> <li>imgpath (list of str): Image path.</li> </ul> </li> </ul>"},{"location":"ncdia/#ncdiatrainersinctrainerpy","title":"ncdia.trainers.inctrainer.py","text":""},{"location":"ncdia/#inctrainer","title":"IncTrainer","text":"<p>IncTrainer class for incremental training.</p> <p>Attributes:</p> <ul> <li>sess_cfg (Configs): Session configuration.</li> <li>num_sess (int): Number of sessions.</li> <li>session (int): Session number. If == 0, execute pre-training.     If &gt; 0, execute incremental training.</li> <li>hist_trainset (MergedDataset): Historical training dataset.</li> <li>hist_valset (MergedDataset): Historical validation dataset.</li> <li>hist_testset (MergedDataset): Historical testing dataset.</li> </ul> <p>Methods:</p> <ul> <li> <p>  __init__(self, cfg=None, sess_cfg=None, ncd_cfg=None, session=0, model=None, hist_trainset=None, hist_testset=None, old_model=None, **kwargs)</p> <p>The constructor method that initializes an instance of IncTrainer. </p> <p>Parameters:</p> <ul> <li>model (nn.Module): Model to be trained.</li> <li>cfg (dict): Configuration for trainer.</li> <li>sess_cfg (Configs): Session configuration.</li> <li>session (int): Session number. Default: 0.</li> </ul> </li> <li> <p> train(self)</p> <p>Incremental training. </p> <p>self.num_sess determines the number of sessions, and session number is stored in self.session.</p> <p>Returns:</p> <ul> <li>model (nn.Module): Trained model.</li> </ul> </li> </ul>"},{"location":"ncdia/#ncdiatrainershooks","title":"ncdia.trainers.hooks","text":"<p>Implements some of the commonly used hooks.</p>"},{"location":"ncdia/#hook","title":"Hook","text":"<p>ncdia.trainers.hooks.hook.py</p> <p>Base hook class. All hooks should inherit from this class.</p>"},{"location":"ncdia/#alghook","title":"AlgHook","text":"<p>ncdia.trainers.hooks.alghook.py</p> <p>A hook to modify algorithm state in the pipeline. This class is a base class for all algorithm hooks.</p>"},{"location":"ncdia/#loggerhook","title":"LoggerHook","text":"<p>ncdia.trainers.hooks.loggerhook.py</p> <p>A hook to log information during training and evaluation.</p>"},{"location":"ncdia/#metrichook","title":"MetricHook","text":"<p>ncdia.trainers.hooks.metrichook.py</p> <p>A hook to calculate metrics during evaluation and testing.</p>"},{"location":"ncdia/#modelhook","title":"ModelHook","text":"<p>ncdia.trainers.hooks.modelhook.py</p> <p>A hook to change model state in the pipeline, such as setting device, changing model to eval mode, etc.</p>"},{"location":"ncdia/#ncdhook","title":"NCDHook","text":"<p>ncdia.trainers.hooks.ncdhook.py</p> <p>A hook to execute OOD and NCD detection to relabel data</p>"},{"location":"ncdia/#optimizerhook","title":"OptimizerHook","text":"<p>ncdia.trainers.hooks.optimizerhook.py</p> <p>A hook to put optimizer to zero_grad and step during training.</p>"},{"location":"ncdia/#schedulerhook","title":"SchedulerHook","text":"<p>ncdia.trainers.hooks.schedulerhook.py</p> <p>A hook to change learning rate during training.</p>"},{"location":"ncdia/#ncdiatrainersoptims","title":"ncdia.trainers.optims","text":""},{"location":"ncdia/#ncdiatrainersoptimsoptimizerpy","title":"ncdia.trainers.optims.optimizer.py","text":"<ul> <li> <p>build_optimizer(type, model, param_groups=None, **kwargs)</p> <p>Build optimizer.</p> <p>Parameters:</p> <ul> <li>type (str): type of optimizer</li> <li>model (nn.Module | dict): model or param_groups</li> <li>param_groups (dict | None):      if provided, directly optimize param_groups and abandon model</li> <li>kwargs (dict): arguments for optimizer</li> </ul> <p>Returns:</p> <ul> <li>optimizer (torch.optim.Optimizer): optimizer</li> </ul> </li> </ul>"},{"location":"ncdia/#ncdiatrainersoptimsschedulerpy","title":"ncdia.trainers.optims.scheduler.py","text":"<p>Implements some of the commonly used scheduler.</p> <ul> <li>CosineWarmupLR</li> <li>LinearWarmupLR</li> <li>ConstantLR</li> </ul> <p>Methods:</p> <ul> <li> <p>build_scheduler(type, optimizer, **kwargs)</p> <p>Build learning rate scheduler.</p> <p>Parameters:</p> <ul> <li>type (str): type of scheduler</li> <li>optimizer (torch.optim.Optimizer): optimizer</li> <li>kwargs (dict): arguments for scheduler</li> </ul> <p>Returns:</p> <ul> <li>lr_scheduler (torch.optim.lr_scheduler._LRScheduler): learning rate scheduler</li> </ul> </li> </ul>"},{"location":"ncdia/#ncdiatrainerspriority","title":"ncdia.trainers.priority","text":"<p>Hook priority levels.</p>"},{"location":"ncdia/#priority","title":"Priority","text":"Level Value HIGHEST 0 VERY_HIGH 10 HIGH 30 ABOVE_NORMAL 40 NORMAL 50 BELOW_NORMAL 60 LOW 70 VERY_LOW 90 LOWEST 100"},{"location":"ncdia/#ncdiautils","title":"ncdia.utils","text":""},{"location":"ncdia/#ncdiautilsmetrics","title":"ncdia.utils.metrics","text":""},{"location":"ncdia/#ncdiautilsmetricsaccuracypy","title":"ncdia.utils.metrics.accuracy.py","text":"<ul> <li> <p>accuracy(output, target, topk=(1,))</p> <p>Computes the accuracy over the k-top predictions for the specified values of k.</p> <p>Parameters:</p> <ul> <li>output (torch.Tensor): model output, shape (batch_size, num_classes)</li> <li>target (torch.Tensor): target labels, shape (batch_size)</li> <li>topk (tuple): top-k values, default is (1,)</li> </ul> <p>Returns:</p> <ul> <li>acc (list): accuracy values for each k in topk</li> </ul> </li> <li> <p>per_class_accuracy(output, target, topk=(1, ))</p> <p>Compute per class accuracy over the k-top predictions for the specified values of k </p> <p>Parameters:</p> <ul> <li>output (torch.Tensor): model output, shape (batch_size, num_classes)</li> <li>target (torch.Tensor): target labels, shape (batch_size)</li> <li>topk (tuple): top-k values, default is (1,)</li> </ul> <p>Returns:</p> <ul> <li>acc (list): accuracy values for each k in topk</li> </ul> </li> </ul>"},{"location":"ncdia/#ncdiautilsmetricsmeterpy","title":"ncdia.utils.metrics.meter.py","text":"<p>AverageMeter</p> <p>Computes and stores the average and current value.</p>"},{"location":"ncdia/#ncdiautilslosses","title":"ncdia.utils.losses","text":""},{"location":"ncdia/#crossentropyloss","title":"CrossEntropyLoss","text":"<p>In crossentropy.py. CrossEntropyLoss with label smoothing.</p>"},{"location":"ncdia/#angularpenaltysmloss","title":"AngularPenaltySMLoss","text":"<p>In angular.py. Angular Penalty Softmax Loss. Three 'loss_types' available: ['arcface', 'sphereface', 'cosface']</p>"},{"location":"ncdia/#ncdiautilscfgpy","title":"ncdia.utils.cfg.py","text":""},{"location":"ncdia/#configs","title":"Configs","text":"<p>Include implementation of setup and use of configs.</p>"},{"location":"ncdia/#ncdiautilsloggerpy","title":"ncdia.utils.logger.py","text":"<p>Include implementation of loggers to write output to console and external text file.</p>"},{"location":"ncdia/#ncdiautilsregistrypy","title":"ncdia.utils.registry.py","text":""},{"location":"ncdia/#registry","title":"Registry","text":"<p>A registry to map strings to classes or functions.</p> <p>Examples:</p> Text Only<pre><code>&gt;&gt;&gt; REGISTRY = Registry()\n&gt;&gt;&gt; @REGISTRY\n&gt;&gt;&gt; def foo():\n&gt;&gt;&gt;     return 'foo'\n&gt;&gt;&gt; @REGISTRY.register\n&gt;&gt;&gt; def bar():\n&gt;&gt;&gt;     return 'bar'\n\n&gt;&gt;&gt; print(REGISTRY['foo']())\nfoo\n&gt;&gt;&gt; print(REGISTRY['bar']())\nbar\n\n&gt;&gt;&gt; print(REGISTRY)\n{'foo': &lt;function foo at 0x7f9b1c0e0d30&gt;, 'bar': &lt;function bar at 0x7f9b1c0e0e18&gt;}\n&gt;&gt;&gt; print(REGISTRY['foo'])\n&lt;function foo at 0x7f9b1c0e0d30&gt;\n&gt;&gt;&gt; print(REGISTRY['bar'])\n&lt;function bar at 0x7f9b1c0e0e18&gt;\n\n&gt;&gt;&gt; print('foo' in REGISTRY)\nTrue\n&gt;&gt;&gt; print('bar' in REGISTRY)\nTrue\n&gt;&gt;&gt; print('foobar' in REGISTRY)\nFalse\n\n&gt;&gt;&gt; print(REGISTRY.keys())\ndict_keys(['foo', 'bar'])\n&gt;&gt;&gt; print(REGISTRY.values())\ndict_values([&lt;function foo at 0x7f9b1c0e0d30&gt;, &lt;function bar at 0x7f9b1c0e0e18&gt;])\n&gt;&gt;&gt; print(REGISTRY.items())\ndict_items([('foo', &lt;function foo at 0x7f9b1c0e0d30&gt;), ('bar', &lt;function bar at 0x7f9b1c0e0e18&gt;)])\n&gt;&gt;&gt; print(len(REGISTRY))\n2\n</code></pre> <ul> <li> <p>register_callable(self, target: callable)</p> <p>Register a target.</p> <p>Parameters:</p> <ul> <li>target (callable): callable target to be registered.</li> </ul> </li> <li> <p>register_dict(self, target)</p> <p>Register a dict.</p> <p>Parameters:</p> <ul> <li>target (dict): A dict to be registered. All its values should be callable.</li> </ul> </li> <li> <p>register(self, target)</p> <p>Register a target.</p> <p>Parameters:</p> <ul> <li>target (callable | dict): target to be registered.</li> </ul> <p>Returns:</p> <ul> <li>target (object): Registered target.</li> </ul> </li> <li> <p>build(self, target: dict | Configs, **kwargs)</p> <p>Build a target with configs.</p> <p>Parameters:</p> <ul> <li>target (dict | Configs): A dict to be built. It should have a key 'type' to specify the target type. It may have other keys to specify the target configs.</li> <li>kwargs (dict): Additional keyword arguments.</li> </ul> <p>Returns:</p> <ul> <li>target (object): A built target.</li> </ul> </li> </ul>"},{"location":"ncdia/#ncdiautilstoolspy","title":"ncdia.utils.tools.py","text":"<ul> <li> <p>mkdir_if_missing(dirname)</p> <p>Create dirname if it is missing.</p> <p>Parameters:</p> <ul> <li>dirname (str): directory path</li> </ul> </li> <li> <p>auto_device(device)</p> <p>Automatically set the device for the input tensor.</p> <p>Parameters:</p> <ul> <li>device (str | torch.device | None): device name or device object. If None, return torch.device('cuda') if available, otherwise return torch.device('cpu').</li> </ul> </li> <li> <p>set_random_seed(seed)</p> <p>Set random seed for reproducibility.</p> <p>Parameters:</p> <ul> <li>seed (int): random seed</li> </ul> </li> </ul>"},{"location":"overview/","title":"\ud83d\udc40 Overview","text":"<p>The framework adopts a modular design overall, which is reflected in two key aspects:</p> <ul> <li>\ud83d\udee0\ufe0f Functionally: The framework independently incorporates dedicated modules for supervised training, out-of-distribution detection, novel class discovery, and incremental learning.</li> <li>\u2699\ufe0f Procedurally\uff1aThe framework divides its operational workflow into distinct stages, including data processing, model construction, training and evaluation, and visualization.</li> </ul> <p>From an implementation perspective, the framework is built upon foundational deep learning, data processing, and visualization libraries such as PyTorch, NumPy, Matplotlib, and Pandas, leveraging their extensive built-in functionalities.</p> <p>This comprehensive framework provides researchers with a robust foundation for tackling open world learning challenges, from initial model training through continuous adaptation to novel scenarios.</p>"},{"location":"people/","title":"\ud83e\uddd1\u200d\ud83e\udd1d\u200d\ud83e\uddd1 People","text":""},{"location":"people/#corresponding-author","title":"\u2b50 Corresponding Author","text":"<p>Xiang Xiang</p> <p></p> <p> Google Scholar |  Homepage | \u2709\ufe0f xex@hust.edu.cn</p> <p>Associate Professor of   Huazhong University of Science and Technology</p> <p>Director of HUST AIA Image and Vision Learning Lab (HAIV Lab)</p> <p>Research Interest: Computer Vision, Open-World Learning</p>"},{"location":"people/#other-authors","title":"\ud83e\udd17 Other Authors","text":"<p>Zhou Qinhao  Huazhong University of Science and Technology</p> <p>Xu Zhuo  Huazhong University of Science and Technology</p> <p>Ma Jing  Huazhong University of Science and Technology</p> <p>Dai Jiaxin  Huazhong University of Science and Technology</p> <p>Liang Yifan  Huazhong University of Science and Technology</p> <p>Li Hanlin  Huazhong University of Science and Technology</p>"},{"location":"people/#special-thanks","title":"\ud83d\ude4f Special Thanks","text":"<p>We would like to express our special gratitude to the following contributors who have graduated but made significant contributions to this project:</p> <p>Zhang Zihan  Huazhong University of Science and Technology</p> <p>Tan Yuwen  Huazhong University of Science and Technology</p> <p>Deng Yao  Huazhong University of Science and Technology</p> <p>Chen Zhipeng  Huazhong University of Science and Technology</p>"},{"location":"quik/","title":"\ud83d\ude80 Quick Start","text":""},{"location":"quik/#installation","title":"\ud83d\udce6 Installation","text":"<p>It is recommended to use anaconda3 to manage and maintain the python library environment.</p> <ol> <li>Download the .sh file from the anaconda3 website</li> <li>Install anaconda3 with .sh file</li> </ol> Bash<pre><code>bash Anaconda3-2023.03-Linux-x86_64.sh\n</code></pre>"},{"location":"quik/#environment-setup","title":"\ud83d\udd27 Environment Setup","text":"<p>Create and activate a virtual environment: Bash<pre><code>conda create -n openhaiv python=3.10 -y\nconda activate openhaiv\npip install -r requirements.txt\npython setup.py install\n</code></pre></p> <p>Required packages:</p> <ul> <li>pytorch&gt;=1.12.0 torchvision&gt;=0.13.0 (recommend official torch command)</li> <li>numpy&gt;=1.26.4</li> <li>scipy&gt;=1.14.0</li> <li>scikit-learn&gt;=1.5.1</li> </ul>"},{"location":"quik/#running-examples","title":"\ud83c\udfc3\u200d\u2642\ufe0f Running Examples","text":""},{"location":"quik/#out-of-distribution-detection","title":"\ud83d\udea8 Out-of-Distribution Detection","text":"Bash<pre><code>python ncdia/train.py --cfg configs/pipeline/ood_detection/msp/det_oes_rn50_msp_train.yaml --opts device='cuda:0'\n</code></pre>"},{"location":"quik/#class-incremental-learning","title":"\ud83c\udf31 Class-incremental Learning","text":"Bash<pre><code>bash ./scripts/inc_BM200_lwf.sh  \n</code></pre>"},{"location":"quik/#novel-class-discovery","title":"\ud83d\udd0d Novel Class Discovery","text":"Bash<pre><code># Set required parameters\n# - model weight in weight_path\n# - id_txt_file and ood_txt_file\n# - output_dir\n\npython ncd.py\n</code></pre>"},{"location":"results/","title":"\ud83d\udcca Results","text":""},{"location":"results/#open-world-learning","title":"\ud83c\udf0d Open World Learning","text":"<p>Table 1.  The experiments of OWL on OpenEarthSensing dataset. ID Acc and ODD Acc are the in-distribution and out-of-distribution performance, respectively, and Avg denotes the average performance of each session.</p> OOD Method CIL Method ID Acc OOD Acc Session 1 Session 2 Avg MSP LwF 91.17 55.01 91.27 42.11 66.69 EWC 91.17 55.01 91.27 28.89 60.08 iCaRL 91.17 55.01 91.27 50.29 70.78 MLS LwF 90.6 63.85 91.27 44.09 67.68 EWC 90.6 63.85 91.27 29.83 60.55 iCaRL 90.6 63.85 91.27 51.33 71.30 VIM LwF 93.5 59.99 91.27 43.49 67.38 EWC 93.5 59.99 91.27 31.33 61.30 iCaRL 93.5 59.99 91.27 33.78 62.52"},{"location":"results/#incremental-learning","title":"\ud83c\udf31 Incremental Learning","text":"<p>Fig 1. Experimental results of CIL. The left figure presents results in randomized order, while the right figure displays systematically organized results arranged from coarse to fine granularity.</p> <p></p>"},{"location":"results/#few-shot-class-incremental-learning","title":"\ud83e\udde9 Few-Shot Class-Incremental Learning","text":"<p>Table 2. The experimental results of few-shot class-incremental learning on the OpenEarthSensing dataset. Shots denote the training samples for each category.</p> 50-shot 10-shot 5-shot 1-shot Last Avg Last Avg Last Avg Last Avg Alice 59.54 64.66 59.17 68.64 58.82 68.35 58.94 68.4 FACT 46.42 49.21 46.38 49.15 46.36 49.15 46.37 49.15 SAVC 71.71 79.55 72.23 80.07 66.61 75.17 59.63 76.77"},{"location":"results/#out-of-distribution-detection","title":"\ud83d\udea8 Out-of-Distribution Detection","text":""},{"location":"results/#cnn-based-methods","title":"CNN-based Methods","text":"<p>Table 3. OOD detection performance on OES benchmark. 'Near' represents the average AUROC for Near-OOD datasets, 'Far' indicates the average AUROC for Far-OOD datasets.</p> Method Standard Res Bias Aerial MSRGB IR Near Far Near Far Near Far Near Far Near Far MSP 88.42 93.91 66.51 78.4 54.38 56.85 65.50 66.92 61.47 65.35 ODIN 87.14 95.79 67.09 75.2 52.85 57.04 66.55 61.55 62.11 73.28 MDS 83.15 96.54 53.86 84.71 48.79 54.76 66.31 81.78 83.64 57.74 MLS 88.59 96.12 66.44 83.17 53.93 59.78 64.46 63.37 62.49 67.06 VIM 90.35 98.75 60.33 83.93 50.69 59.72 64.90 81.75 57.65 51.08 FBDB 90.24 98.17 66.64 87.87 54.49 60.41 59.45 74.49 61.40 68.62 VOS 86.19 95.68 63.37 81.32 51.10 60.01 59.77 58.72 59.47 60.26 LogiNorm 89.00 95.15 68.80 80.29 53.25 56.72 77.69 55.43 64.12 63.97 DML 84.38 90.36 65.78 76.16 52.89 58.60 62.56 50.68 60.39 50.56"},{"location":"results/#clip-based-methods","title":"CLIP-based Methods","text":"<p>Table 4. CLIP based methods' performance on OES benchmark. 'Near' represents the average AUROC for Near-OOD datasets, 'Far' indicates the average AUROC for Far-OOD datasets.</p> Method Standard Res Bias Aerial MSRGB IR Near Far Near Far Near Far Near Far Near Far MaxLogits 53.31 43.95 68.99 63.32 64.73 40.46 68.22 9.34 62.73 37.00 MCM 61.79 52.60 59.97 51.94 66.07 67.85 58.90 55.89 54.41 40.43 GLMCM 62.07 52.33 59.20 51.57 65.20 67.42 57.32 56.89 51.75 42.30 CoOp 86.04 94.21 64.09 73.36 61.22 76.40 66.73 90.22 61.30 45.16 LoCoOp 85.71 90.94 66.20 71.67 64.18 76.52 69.64 86.28 61.41 43.33 SCT 85.56 90.78 65.37 70.30 64.14 77.67 68.58 86.41 60.81 42.48 DPM 91.19 99.24 68.60 92.61 60.50 71.26 74.66 93.56 65.11 75.10"},{"location":"scripts/","title":"\ud83d\udcdc Scripts","text":"<p>The OpenHAIV framework provides a comprehensive set of shell scripts that help users quickly execute various experiments. These scripts are organized by task type and provide a convenient way to run experiments with pre-configured settings. This document explains the script directory structure, naming conventions, and how to use and customize these scripts.</p>"},{"location":"scripts/#script-directory-structure","title":"Script Directory Structure","text":"<p>The scripts are organized in the following directory structure:</p> Text Only<pre><code>scripts/\n\u251c\u2500\u2500 sl/           # Supervised Learning scripts\n\u251c\u2500\u2500 inc/          # Incremental Learning scripts\n\u251c\u2500\u2500 ood/          # Out-of-Distribution Detection scripts\n\u2514\u2500\u2500 ncdia_*.sh    # Neural Component Decoupling scripts\n</code></pre>"},{"location":"scripts/#naming-conventions","title":"Naming Conventions","text":"<p>Scripts follow a consistent naming pattern that indicates: 1. The task type (e.g., <code>sl_</code>, <code>inc_</code>, <code>det_</code>) 2. The dataset (e.g., <code>oes_</code>, <code>cifar10_</code>, <code>cub_</code>) 3. The model architecture (e.g., <code>rn18</code>, <code>coop-b16</code>) 4. The algorithm or method (e.g., <code>msp</code>, <code>lwf</code>, <code>savc</code>)</p> <p>Example: <code>sl_oes_coop-b16.sh</code> indicates a Supervised Learning task on the OES dataset using CoOp-CLIP-B/16 model.</p>"},{"location":"scripts/#script-types","title":"Script Types","text":""},{"location":"scripts/#supervised-learning-scripts-sl","title":"Supervised Learning Scripts (<code>sl/</code>)","text":"<p>These scripts train models in a standard supervised learning setting. They typically use the <code>PreTrainer</code> trainer and standard classification losses.</p> <p>Example: Bash<pre><code># Benchmark: OES\n# Model: ResNet18\n# Method: Cross-Entropy\n# Task: Supervised Learning\npython ncdia/train.py \\\n    --cfg configs/pipeline/supervised_learning/sl_oes_rn18.yaml \\\n    --opts device='cuda:0'\n</code></pre></p> <p>Common supervised learning scripts include: - <code>sl_oes_rn18.sh</code>: Training a ResNet18 model on OES dataset - <code>sl_cifar10_rn18.sh</code>: Training a ResNet18 model on CIFAR10 dataset - <code>sl_oes_coop-b16.sh</code>: Training a CoOp-CLIP-B/16 model on OES dataset</p>"},{"location":"scripts/#incremental-learning-scripts-inc","title":"Incremental Learning Scripts (<code>inc/</code>)","text":"<p>These scripts train models in an incremental learning setting, where new classes are introduced over time. There are two main subtypes:</p>"},{"location":"scripts/#class-incremental-learning","title":"Class-incremental Learning","text":"<p>Example: Bash<pre><code># Benchmark: CUB200\n# Model: ResNet18\n# Method: LwF\n# Task: Class-incremental Learning\npython ncdia/train.py \\\n    --cfg configs/pipeline/incremental_learning/inc_cub_lwf.yaml \\\n    --opts device='cuda:0'\n</code></pre></p>"},{"location":"scripts/#few-shot-class-incremental-learning","title":"Few-shot Class-incremental Learning","text":"<p>Example: Bash<pre><code># Benchmark: BM200\n# Model: ResNet18\n# Method: SAVC\n# Task: Few-shot Class-incremental Learning\npython ncdia/train.py \\\n    --cfg configs/pipeline/incremental_learning/inc_BM200_savc.yaml \\\n    --opts device='cuda:0'\n</code></pre></p> <p>Common incremental learning scripts include: - <code>inc_cub_lwf.sh</code>: Learning without Forgetting on CUB200 dataset - <code>inc_cub_icarl.sh</code>: iCaRL method on CUB200 dataset - <code>inc_BM200_savc.sh</code>: SAVC method on BM200 dataset (few-shot setting)</p>"},{"location":"scripts/#out-of-distribution-detection-scripts-ood","title":"Out-of-Distribution Detection Scripts (<code>ood/</code>)","text":"<p>These scripts train and evaluate models for out-of-distribution detection, identifying samples that don't belong to the training distribution.</p> <p>Example: Bash<pre><code># Benchmark: OES\n# Model: ResNet50\n# Method: MSP\n# Task: Out-of-Distribution Detection\npython ncdia/train.py \\\n    --cfg configs/pipeline/ood_detection/det_oes_rn50_msp.yaml \\\n    --opts device='cuda:0'\n</code></pre></p> <p>Common OOD detection scripts include: - <code>det_oes_rn50_msp.sh</code>: Maximum Softmax Probability method with ResNet50 on OES dataset - <code>det_oes_rn50_mls.sh</code>: Maximum Logit Score method with ResNet50 on OES dataset - <code>det_oes_clip-b16_glmcm.sh</code>: GL-MCM method with CLIP-B/16 on OES dataset</p>"},{"location":"scripts/#batch-processing-scripts","title":"Batch Processing Scripts","text":"<p>Some scripts are designed to run multiple related experiments in sequence:</p> <p>Example: Bash<pre><code># Benchmark: OES\n# Model: Multiple CLIP-based Models\n# Method: Multiple Detection Methods\n# Task: Out-of-Distribution Detection (With Training)\n\nbash scripts/ood/det_oes_coop-b16_mcm.sh\nbash scripts/ood/det_oes_locoop-b16_glmcm.sh\nbash scripts/ood/det_oes_sct-b16_glmcm.sh\nbash scripts/ood/det_oes_dpm-b16_dpm.sh\n# More scripts...\n</code></pre></p>"},{"location":"scripts/#customizing-scripts","title":"Customizing Scripts","text":"<p>You can customize existing scripts or create new ones by following these steps:</p> <ol> <li> <p>Copy an Existing Script:    Bash<pre><code>cp scripts/sl/sl_oes_rn18.sh scripts/sl/sl_oes_mymodel.sh\n</code></pre></p> </li> <li> <p>Modify the Configuration Path:    Bash<pre><code># Change this line\n--cfg configs/pipeline/supervised_learning/sl_oes_rn18.yaml \\\n# To point to your custom configuration\n--cfg configs/pipeline/supervised_learning/sl_oes_mymodel.yaml \\\n</code></pre></p> </li> <li> <p>Update the Script Comments to reflect your experiment:    Bash<pre><code># Benchmark: OES\n# Model: MyModel\n# Method: MyMethod\n# Task: Supervised Learning\n</code></pre></p> </li> </ol>"},{"location":"scripts/#running-scripts","title":"Running Scripts","text":"<p>To run a script, simply execute it with bash:</p> Bash<pre><code>bash scripts/sl/sl_oes_rn18.sh\n</code></pre> <p>You can also override configuration parameters directly from the command line:</p> Bash<pre><code>bash scripts/sl/sl_oes_rn18.sh --opts device='cuda:1' trainer.max_epochs=20\n</code></pre>"},{"location":"scripts/#advanced-usage","title":"Advanced Usage","text":""},{"location":"scripts/#gpu-selection","title":"GPU Selection","text":"<p>Most scripts allow specifying the GPU device using the <code>device</code> parameter:</p> Bash<pre><code>python ncdia/train.py \\\n    --cfg configs/pipeline/supervised_learning/sl_oes_rn18.yaml \\\n    --opts device='cuda:0'  # Use the first GPU\n</code></pre> <p>To use a different GPU, change <code>cuda:0</code> to the desired GPU index (e.g., <code>cuda:1</code>).</p>"},{"location":"scripts/#multi-stage-training","title":"Multi-stage Training","text":"<p>Some scripts implement multi-stage training, where different phases (e.g., training, testing) are executed sequentially:</p> Bash<pre><code># Training phase\npython ncdia/train.py \\\n    --cfg configs/pipeline/ood_detection/msp/det_oes_rn50_msp_train.yaml \\\n    --opts device='cuda:0'\n\n# Testing phase\npython ncdia/train.py \\\n    --cfg configs/pipeline/ood_detection/msp/det_oes_rn50_msp_test.yaml \\\n    --opts device='cuda:0'\n</code></pre>"},{"location":"scripts/#running-on-multiple-gpus","title":"Running on Multiple GPUs","text":"<p>For multi-GPU training, you can use the <code>CUDA_VISIBLE_DEVICES</code> environment variable:</p> Bash<pre><code>CUDA_VISIBLE_DEVICES=0,1,2,3 python ncdia/train.py \\\n    --cfg configs/pipeline/supervised_learning/sl_oes_rn18.yaml \\\n    --opts device='cuda' trainer.distributed=True\n</code></pre>"}]}