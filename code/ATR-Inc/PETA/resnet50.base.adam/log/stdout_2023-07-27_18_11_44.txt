BACKBONE:
  MULTISCALE: False
  TYPE: convformer_s36.sail_in22k_ft_in1k
CLASSIFIER:
  BN: False
  NAME: linear
  POOLING: avg
  SCALE: 1
  TYPE: base
DATASET:
  HEIGHT: 256
  LABEL: eval
  NAME: PETA
  TARGETTRANSFORM: []
  TEST_SPLIT: test
  TRAIN_SPLIT: trainval
  TYPE: pedes
  VAL_SPLIT: test
  WIDTH: 192
  ZERO_SHOT: False
DISTRIBUTTED: False
INFER:
  SAMPLING: False
LOSS:
  LOSS_WEIGHT: [1]
  SAMPLE_WEIGHT: weight
  SIZESUM: True
  TYPE: bceloss
METRIC:
  TYPE: pedestrian
NAME: resnet50.base.adam
REDIRECTOR: True
RELOAD:
  NAME: backbone
  PTH: 
  TYPE: False
TRAIN:
  AUX_LOSS_START: -1
  BATCH_SIZE: 64
  BN_WD: True
  CLIP_GRAD: True
  DATAAUG:
    AUTOAUG_PROB: 0.5
    TYPE: base
  EMA:
    DECAY: 0.9998
    ENABLE: False
    FORCE_CPU: False
  LR_SCHEDULER:
    LR_FT: 0.0001
    LR_NEW: 0.0001
    LR_STEP: [0]
    TYPE: annealing_cosine
    WMUP_COEF: 0.1
  MAX_EPOCH: 30
  NUM_WORKERS: 4
  OPTIMIZER:
    MOMENTUM: 0.9
    TYPE: adam
    WEIGHT_DECAY: 0.0005
  SHUFFLE: True
TRANS:
  DEC_LAYERS: 6
  DIM_FFD: 2048
  DIM_HIDDEN: 256
  DROPOUT: 0.1
  ENC_LAYERS: 6
  EOS_COEF: 0.1
  NHEADS: 8
  NUM_QUERIES: 100
  PRE_NORM: False
VIS:
  CAM: valid
  TENSORBOARD:
    ENABLE: True
  VISDOM: False
Compose(
    Resize(size=(256, 192), interpolation=bilinear, max_size=None, antialias=None)
    Pad(padding=10, fill=0, padding_mode=constant)
    RandomCrop(size=(256, 192), padding=None)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
backbone: convformer_s36.sail_in22k_ft_in1k, classifier: linear
model_name: resnet50.base.adam
2023-07-27_18:15:05, Step 99/1248 in Ep 0, LR: [1.0e-04, 1.0e-04] Time: 1.88s , train_loss: 33.1200, 
['0.0000']
2023-07-27_18:18:16, Step 199/1248 in Ep 0, LR: [1.0e-04, 1.0e-04] Time: 2.00s , train_loss: 27.7312, 
['0.0000']
2023-07-27_18:21:26, Step 299/1248 in Ep 0, LR: [1.0e-04, 1.0e-04] Time: 1.87s , train_loss: 25.1291, 
['0.0000']
2023-07-27_18:24:38, Step 399/1248 in Ep 0, LR: [1.0e-04, 1.0e-04] Time: 1.89s , train_loss: 23.4489, 
['0.0000']
2023-07-27_18:27:50, Step 499/1248 in Ep 0, LR: [1.0e-04, 1.0e-04] Time: 1.87s , train_loss: 22.2830, 
['0.0000']
2023-07-27_18:31:03, Step 599/1248 in Ep 0, LR: [1.0e-04, 1.0e-04] Time: 1.92s , train_loss: 21.4615, 
['0.0000']
2023-07-27_18:34:15, Step 699/1248 in Ep 0, LR: [1.0e-04, 1.0e-04] Time: 1.98s , train_loss: 20.7446, 
['0.0000']
2023-07-27_18:37:27, Step 799/1248 in Ep 0, LR: [1.0e-04, 1.0e-04] Time: 1.88s , train_loss: 20.1723, 
['0.0000']
2023-07-27_18:40:38, Step 899/1248 in Ep 0, LR: [1.0e-04, 1.0e-04] Time: 1.89s , train_loss: 19.7191, 
['0.0000']
2023-07-27_18:43:49, Step 999/1248 in Ep 0, LR: [1.0e-04, 1.0e-04] Time: 1.87s , train_loss: 19.2973, 
['0.0000']
2023-07-27_18:47:04, Step 1099/1248 in Ep 0, LR: [1.0e-04, 1.0e-04] Time: 1.87s , train_loss: 18.9289, 
['0.0000']
2023-07-27_18:50:16, Step 1199/1248 in Ep 0, LR: [1.0e-04, 1.0e-04] Time: 1.89s , train_loss: 18.6188, 
['0.0000']
2023-07-27_18:51:48, Step 1247/1248 in Ep 0, LR: [1.0e-04, 1.0e-04] Time: 1.89s , train_loss: 18.4758, 
['0.0000']
Epoch 0, LR 0.0001, Train_Time 2398.32s, Loss: 18.4758
['0.0000']
Evaluation on train set, train losses 18.475844799708096
 ma: 0.7533, label_f1: 0.5554, pos_recall: 0.5550 , neg_recall: 0.9516 
 Acc: 0.7470, Prec: 0.8449, Rec: 0.8526, F1: 0.8487
Evaluation on test set, valid losses 13.9115422706604
 ma: 0.8074, label_f1: 0.6435, pos_recall: 0.6527 , neg_recall: 0.9621 
 Acc: 0.8044, Prec: 0.8774, Rec: 0.8982, F1: 0.8877
2023-07-27_18:53:05
------------------------------------------------------------
2023-07-27_18:56:20, Step 99/1248 in Ep 1, LR: [1.0e-04, 1.0e-04] Time: 1.90s , train_loss: 13.6784, 
['0.0000']
2023-07-27_18:59:33, Step 199/1248 in Ep 1, LR: [1.0e-04, 1.0e-04] Time: 1.94s , train_loss: 13.7165, 
['0.0000']
2023-07-27_19:02:45, Step 299/1248 in Ep 1, LR: [1.0e-04, 1.0e-04] Time: 1.95s , train_loss: 13.7131, 
['0.0000']
2023-07-27_19:05:58, Step 399/1248 in Ep 1, LR: [1.0e-04, 1.0e-04] Time: 1.90s , train_loss: 13.6762, 
['0.0000']
2023-07-27_19:09:17, Step 499/1248 in Ep 1, LR: [1.0e-04, 1.0e-04] Time: 1.98s , train_loss: 13.6330, 
['0.0000']
2023-07-27_19:12:31, Step 599/1248 in Ep 1, LR: [1.0e-04, 1.0e-04] Time: 1.90s , train_loss: 13.5973, 
['0.0000']
2023-07-27_19:15:46, Step 699/1248 in Ep 1, LR: [1.0e-04, 1.0e-04] Time: 1.93s , train_loss: 13.5775, 
['0.0000']
2023-07-27_19:19:00, Step 799/1248 in Ep 1, LR: [9.9e-05, 9.9e-05] Time: 2.09s , train_loss: 13.5661, 
['0.0000']
2023-07-27_19:22:14, Step 899/1248 in Ep 1, LR: [9.9e-05, 9.9e-05] Time: 1.90s , train_loss: 13.5495, 
['0.0000']
2023-07-27_19:25:28, Step 999/1248 in Ep 1, LR: [9.9e-05, 9.9e-05] Time: 1.91s , train_loss: 13.5168, 
['0.0000']
2023-07-27_19:28:42, Step 1099/1248 in Ep 1, LR: [9.9e-05, 9.9e-05] Time: 2.02s , train_loss: 13.4746, 
['0.0000']
2023-07-27_19:32:02, Step 1199/1248 in Ep 1, LR: [9.9e-05, 9.9e-05] Time: 1.90s , train_loss: 13.4448, 
['0.0000']
2023-07-27_19:33:36, Step 1247/1248 in Ep 1, LR: [9.9e-05, 9.9e-05] Time: 1.89s , train_loss: 13.4347, 
['0.0000']
Epoch 1, LR 9.97989159850622e-05, Train_Time 2430.60s, Loss: 13.4347
['0.0000']
Evaluation on train set, train losses 13.434698937795101
 ma: 0.8181, label_f1: 0.6659, pos_recall: 0.6721 , neg_recall: 0.9641 
 Acc: 0.8096, Prec: 0.8807, Rec: 0.9008, F1: 0.8906
Evaluation on test set, valid losses 11.930740112304688
 ma: 0.8359, label_f1: 0.7003, pos_recall: 0.7050 , neg_recall: 0.9669 
 Acc: 0.8323, Prec: 0.8925, Rec: 0.9178, F1: 0.9050
2023-07-27_19:34:53
------------------------------------------------------------
2023-07-27_19:38:12, Step 99/1248 in Ep 2, LR: [9.9e-05, 9.9e-05] Time: 1.93s , train_loss: 11.8333, 
['0.0000']
2023-07-27_19:41:27, Step 199/1248 in Ep 2, LR: [9.9e-05, 9.9e-05] Time: 1.90s , train_loss: 11.8492, 
['0.0000']
2023-07-27_19:44:44, Step 299/1248 in Ep 2, LR: [9.9e-05, 9.9e-05] Time: 2.00s , train_loss: 11.7831, 
['0.0000']
2023-07-27_19:48:19, Step 399/1248 in Ep 2, LR: [9.9e-05, 9.9e-05] Time: 4.99s , train_loss: 11.8178, 
['0.0000']
2023-07-27_19:52:38, Step 499/1248 in Ep 2, LR: [9.9e-05, 9.9e-05] Time: 2.00s , train_loss: 11.8912, 
['0.0000']
2023-07-27_19:55:56, Step 599/1248 in Ep 2, LR: [9.9e-05, 9.9e-05] Time: 1.89s , train_loss: 11.9135, 
['0.0000']
2023-07-27_19:59:10, Step 699/1248 in Ep 2, LR: [9.9e-05, 9.9e-05] Time: 1.92s , train_loss: 11.9420, 
['0.0000']
2023-07-27_20:02:23, Step 799/1248 in Ep 2, LR: [9.9e-05, 9.9e-05] Time: 1.99s , train_loss: 11.9517, 
['0.0000']
2023-07-27_20:05:35, Step 899/1248 in Ep 2, LR: [9.9e-05, 9.9e-05] Time: 1.90s , train_loss: 11.9617, 
['0.0000']
2023-07-27_20:08:47, Step 999/1248 in Ep 2, LR: [9.8e-05, 9.8e-05] Time: 1.99s , train_loss: 11.9605, 
['0.0000']
2023-07-27_20:11:59, Step 1099/1248 in Ep 2, LR: [9.8e-05, 9.8e-05] Time: 1.92s , train_loss: 11.9635, 
['0.0000']
2023-07-27_20:15:11, Step 1199/1248 in Ep 2, LR: [9.8e-05, 9.8e-05] Time: 1.90s , train_loss: 11.9652, 
['0.0000']
2023-07-27_20:16:44, Step 1247/1248 in Ep 2, LR: [9.8e-05, 9.8e-05] Time: 1.99s , train_loss: 11.9670, 
['0.0000']
Epoch 2, LR 9.919728295050156e-05, Train_Time 2509.00s, Loss: 11.9670
['0.0000']
Evaluation on train set, train losses 11.967004727858763
 ma: 0.8382, label_f1: 0.7003, pos_recall: 0.7086 , neg_recall: 0.9678 
 Acc: 0.8291, Prec: 0.8921, Rec: 0.9136, F1: 0.9027
Evaluation on test set, valid losses 10.687928646087647
 ma: 0.8533, label_f1: 0.7250, pos_recall: 0.7348 , neg_recall: 0.9717 
 Acc: 0.8463, Prec: 0.9038, Rec: 0.9232, F1: 0.9134
2023-07-27_20:18:36
------------------------------------------------------------
2023-07-27_20:21:54, Step 99/1248 in Ep 3, LR: [9.8e-05, 9.8e-05] Time: 1.94s , train_loss: 10.5156, 
['0.0000']
2023-07-27_20:25:06, Step 199/1248 in Ep 3, LR: [9.8e-05, 9.8e-05] Time: 1.90s , train_loss: 10.5850, 
['0.0000']
2023-07-27_20:28:19, Step 299/1248 in Ep 3, LR: [9.8e-05, 9.8e-05] Time: 1.93s , train_loss: 10.6161, 
['0.0000']
2023-07-27_20:31:32, Step 399/1248 in Ep 3, LR: [9.8e-05, 9.8e-05] Time: 1.87s , train_loss: 10.6747, 
['0.0000']
2023-07-27_20:34:44, Step 499/1248 in Ep 3, LR: [9.8e-05, 9.8e-05] Time: 1.86s , train_loss: 10.7427, 
['0.0000']
2023-07-27_20:37:56, Step 599/1248 in Ep 3, LR: [9.8e-05, 9.8e-05] Time: 1.88s , train_loss: 10.7780, 
['0.0000']
2023-07-27_20:41:11, Step 699/1248 in Ep 3, LR: [9.7e-05, 9.7e-05] Time: 2.06s , train_loss: 10.8027, 
['0.0000']
2023-07-27_20:44:25, Step 799/1248 in Ep 3, LR: [9.7e-05, 9.7e-05] Time: 1.93s , train_loss: 10.8349, 
['0.0000']
2023-07-27_20:47:38, Step 899/1248 in Ep 3, LR: [9.7e-05, 9.7e-05] Time: 1.86s , train_loss: 10.8570, 
['0.0000']
2023-07-27_20:50:50, Step 999/1248 in Ep 3, LR: [9.7e-05, 9.7e-05] Time: 1.90s , train_loss: 10.8850, 
['0.0000']
2023-07-27_20:54:02, Step 1099/1248 in Ep 3, LR: [9.7e-05, 9.7e-05] Time: 1.90s , train_loss: 10.9025, 
['0.0000']
2023-07-27_20:57:14, Step 1199/1248 in Ep 3, LR: [9.7e-05, 9.7e-05] Time: 1.89s , train_loss: 10.9198, 
['0.0000']
2023-07-27_20:58:46, Step 1247/1248 in Ep 3, LR: [9.7e-05, 9.7e-05] Time: 1.86s , train_loss: 10.9280, 
['0.0000']
Epoch 3, LR 9.819994489175787e-05, Train_Time 2407.75s, Loss: 10.9280
['0.0000']
Evaluation on train set, train losses 10.927964215095226
 ma: 0.8507, label_f1: 0.7223, pos_recall: 0.7309 , neg_recall: 0.9706 
 Acc: 0.8436, Prec: 0.9011, Rec: 0.9224, F1: 0.9116
Evaluation on test set, valid losses 9.507952081680298
 ma: 0.8577, label_f1: 0.7355, pos_recall: 0.7404 , neg_recall: 0.9750 
 Acc: 0.8623, Prec: 0.9130, Rec: 0.9333, F1: 0.9230
2023-07-27_21:00:04
------------------------------------------------------------
2023-07-27_21:03:22, Step 99/1248 in Ep 4, LR: [9.7e-05, 9.7e-05] Time: 1.92s , train_loss: 9.6330, 
['0.0000']
2023-07-27_21:06:40, Step 199/1248 in Ep 4, LR: [9.7e-05, 9.7e-05] Time: 2.02s , train_loss: 9.6770, 
['0.0000']
2023-07-27_21:09:52, Step 299/1248 in Ep 4, LR: [9.6e-05, 9.6e-05] Time: 1.90s , train_loss: 9.7809, 
['0.0000']
2023-07-27_21:13:05, Step 399/1248 in Ep 4, LR: [9.6e-05, 9.6e-05] Time: 1.94s , train_loss: 9.8326, 
['0.0000']
2023-07-27_21:16:17, Step 499/1248 in Ep 4, LR: [9.6e-05, 9.6e-05] Time: 1.90s , train_loss: 9.8541, 
['0.0000']
2023-07-27_21:19:29, Step 599/1248 in Ep 4, LR: [9.6e-05, 9.6e-05] Time: 1.89s , train_loss: 9.9036, 
['0.0000']
2023-07-27_21:22:41, Step 699/1248 in Ep 4, LR: [9.6e-05, 9.6e-05] Time: 1.94s , train_loss: 9.9220, 
['0.0000']
2023-07-27_21:25:53, Step 799/1248 in Ep 4, LR: [9.6e-05, 9.6e-05] Time: 1.89s , train_loss: 9.9658, 
['0.0000']
2023-07-27_21:29:10, Step 899/1248 in Ep 4, LR: [9.6e-05, 9.6e-05] Time: 1.91s , train_loss: 9.9787, 
['0.0000']
2023-07-27_21:32:24, Step 999/1248 in Ep 4, LR: [9.5e-05, 9.5e-05] Time: 1.92s , train_loss: 10.0083, 
['0.0000']
2023-07-27_21:35:40, Step 1099/1248 in Ep 4, LR: [9.5e-05, 9.5e-05] Time: 1.92s , train_loss: 10.0374, 
['0.0000']
2023-07-27_21:38:56, Step 1199/1248 in Ep 4, LR: [9.5e-05, 9.5e-05] Time: 1.96s , train_loss: 10.0610, 
['0.0000']
2023-07-27_21:40:30, Step 1247/1248 in Ep 4, LR: [9.5e-05, 9.5e-05] Time: 1.98s , train_loss: 10.0650, 
['0.0000']
Epoch 4, LR 9.681493178845488e-05, Train_Time 2422.70s, Loss: 10.0650
['0.0000']
Evaluation on train set, train losses 10.06501060571426
 ma: 0.8597, label_f1: 0.7389, pos_recall: 0.7464 , neg_recall: 0.9730 
 Acc: 0.8554, Prec: 0.9087, Rec: 0.9290, F1: 0.9187
Evaluation on test set, valid losses 8.814338256835937
 ma: 0.8710, label_f1: 0.7536, pos_recall: 0.7666 , neg_recall: 0.9753 
 Acc: 0.8722, Prec: 0.9168, Rec: 0.9414, F1: 0.9290
2023-07-27_21:41:49
------------------------------------------------------------
2023-07-27_21:45:11, Step 99/1248 in Ep 5, LR: [9.5e-05, 9.5e-05] Time: 1.90s , train_loss: 8.7861, 
['0.0000']
2023-07-27_21:48:27, Step 199/1248 in Ep 5, LR: [9.5e-05, 9.5e-05] Time: 1.91s , train_loss: 8.9369, 
['0.0000']
2023-07-27_21:51:47, Step 299/1248 in Ep 5, LR: [9.5e-05, 9.5e-05] Time: 1.97s , train_loss: 8.9874, 
['0.0000']
2023-07-27_21:55:04, Step 399/1248 in Ep 5, LR: [9.4e-05, 9.4e-05] Time: 1.96s , train_loss: 9.0653, 
['0.0000']
2023-07-27_21:58:20, Step 499/1248 in Ep 5, LR: [9.4e-05, 9.4e-05] Time: 1.96s , train_loss: 9.1076, 
['0.0000']
2023-07-27_22:01:36, Step 599/1248 in Ep 5, LR: [9.4e-05, 9.4e-05] Time: 2.08s , train_loss: 9.1621, 
['0.0000']
2023-07-27_22:04:53, Step 699/1248 in Ep 5, LR: [9.4e-05, 9.4e-05] Time: 1.92s , train_loss: 9.1922, 
['0.0000']
2023-07-27_22:08:10, Step 799/1248 in Ep 5, LR: [9.4e-05, 9.4e-05] Time: 1.94s , train_loss: 9.2439, 
['0.0000']
2023-07-27_22:11:27, Step 899/1248 in Ep 5, LR: [9.4e-05, 9.4e-05] Time: 1.97s , train_loss: 9.2481, 
['0.0000']
2023-07-27_22:14:47, Step 999/1248 in Ep 5, LR: [9.3e-05, 9.3e-05] Time: 1.94s , train_loss: 9.2779, 
['0.0000']
2023-07-27_22:18:05, Step 1099/1248 in Ep 5, LR: [9.3e-05, 9.3e-05] Time: 1.94s , train_loss: 9.3018, 
['0.0000']
2023-07-27_22:21:22, Step 1199/1248 in Ep 5, LR: [9.3e-05, 9.3e-05] Time: 1.93s , train_loss: 9.3076, 
['0.0000']
2023-07-27_22:22:56, Step 1247/1248 in Ep 5, LR: [9.3e-05, 9.3e-05] Time: 1.95s , train_loss: 9.3156, 
['0.0000']
Epoch 5, LR 9.505339495172584e-05, Train_Time 2463.66s, Loss: 9.3156
['0.0000']
Evaluation on train set, train losses 9.31558466874636
 ma: 0.8656, label_f1: 0.7502, pos_recall: 0.7561 , neg_recall: 0.9752 
 Acc: 0.8664, Prec: 0.9160, Rec: 0.9348, F1: 0.9253
Evaluation on test set, valid losses 8.336602041244507
 ma: 0.8759, label_f1: 0.7594, pos_recall: 0.7739 , neg_recall: 0.9778 
 Acc: 0.8785, Prec: 0.9225, Rec: 0.9429, F1: 0.9326
2023-07-27_22:24:15
------------------------------------------------------------
2023-07-27_22:27:37, Step 99/1248 in Ep 6, LR: [9.3e-05, 9.3e-05] Time: 1.89s , train_loss: 8.3230, 
['0.0000']
2023-07-27_22:30:54, Step 199/1248 in Ep 6, LR: [9.3e-05, 9.3e-05] Time: 2.07s , train_loss: 8.3265, 
['0.0000']
2023-07-27_22:34:11, Step 299/1248 in Ep 6, LR: [9.2e-05, 9.2e-05] Time: 1.91s , train_loss: 8.3627, 
['0.0000']
2023-07-27_22:37:33, Step 399/1248 in Ep 6, LR: [9.2e-05, 9.2e-05] Time: 1.89s , train_loss: 8.3941, 
['0.0000']
2023-07-27_22:40:50, Step 499/1248 in Ep 6, LR: [9.2e-05, 9.2e-05] Time: 2.04s , train_loss: 8.4435, 
['0.0000']
2023-07-27_22:44:06, Step 599/1248 in Ep 6, LR: [9.2e-05, 9.2e-05] Time: 1.90s , train_loss: 8.4789, 
['0.0000']
2023-07-27_22:47:23, Step 699/1248 in Ep 6, LR: [9.2e-05, 9.2e-05] Time: 1.88s , train_loss: 8.4965, 
['0.0000']
2023-07-27_22:50:39, Step 799/1248 in Ep 6, LR: [9.1e-05, 9.1e-05] Time: 1.94s , train_loss: 8.5201, 
['0.0000']
2023-07-27_22:53:55, Step 899/1248 in Ep 6, LR: [9.1e-05, 9.1e-05] Time: 1.92s , train_loss: 8.5634, 
['0.0000']
2023-07-27_22:57:12, Step 999/1248 in Ep 6, LR: [9.1e-05, 9.1e-05] Time: 1.94s , train_loss: 8.5923, 
['0.0000']
2023-07-27_23:00:33, Step 1099/1248 in Ep 6, LR: [9.1e-05, 9.1e-05] Time: 1.93s , train_loss: 8.6328, 
['0.0000']
2023-07-27_23:03:50, Step 1199/1248 in Ep 6, LR: [9.1e-05, 9.1e-05] Time: 1.88s , train_loss: 8.6600, 
['0.0000']
2023-07-27_23:05:24, Step 1247/1248 in Ep 6, LR: [9.0e-05, 9.0e-05] Time: 2.10s , train_loss: 8.6794, 
['0.0000']
Epoch 6, LR 9.292951724041321e-05, Train_Time 2465.48s, Loss: 8.6794
['0.0000']
Evaluation on train set, train losses 8.67943795139973
 ma: 0.8741, label_f1: 0.7649, pos_recall: 0.7711 , neg_recall: 0.9771 
 Acc: 0.8755, Prec: 0.9219, Rec: 0.9397, F1: 0.9307
Evaluation on test set, valid losses 7.692720792770386
 ma: 0.8818, label_f1: 0.7713, pos_recall: 0.7846 , neg_recall: 0.9790 
 Acc: 0.8905, Prec: 0.9302, Rec: 0.9492, F1: 0.9396
2023-07-27_23:06:45
------------------------------------------------------------
2023-07-27_23:10:08, Step 99/1248 in Ep 7, LR: [9.0e-05, 9.0e-05] Time: 1.92s , train_loss: 7.6162, 
['0.0000']
2023-07-27_23:13:25, Step 199/1248 in Ep 7, LR: [9.0e-05, 9.0e-05] Time: 1.94s , train_loss: 7.6904, 
['0.0000']
2023-07-27_23:16:42, Step 299/1248 in Ep 7, LR: [9.0e-05, 9.0e-05] Time: 1.95s , train_loss: 7.7546, 
['0.0000']
2023-07-27_23:19:58, Step 399/1248 in Ep 7, LR: [9.0e-05, 9.0e-05] Time: 1.99s , train_loss: 7.7991, 
['0.0000']
2023-07-27_23:23:19, Step 499/1248 in Ep 7, LR: [8.9e-05, 8.9e-05] Time: 1.93s , train_loss: 7.8597, 
['0.0000']
2023-07-27_23:26:36, Step 599/1248 in Ep 7, LR: [8.9e-05, 8.9e-05] Time: 1.92s , train_loss: 7.8905, 
['0.0000']
2023-07-27_23:29:53, Step 699/1248 in Ep 7, LR: [8.9e-05, 8.9e-05] Time: 1.89s , train_loss: 7.9319, 
['0.0000']
2023-07-27_23:33:10, Step 799/1248 in Ep 7, LR: [8.9e-05, 8.9e-05] Time: 2.15s , train_loss: 7.9754, 
['0.0000']
2023-07-27_23:36:26, Step 899/1248 in Ep 7, LR: [8.8e-05, 8.8e-05] Time: 1.96s , train_loss: 7.9977, 
['0.0000']
2023-07-27_23:39:43, Step 999/1248 in Ep 7, LR: [8.8e-05, 8.8e-05] Time: 1.96s , train_loss: 8.0284, 
['0.0000']
2023-07-27_23:43:00, Step 1099/1248 in Ep 7, LR: [8.8e-05, 8.8e-05] Time: 1.94s , train_loss: 8.0592, 
['0.0000']
2023-07-27_23:46:21, Step 1199/1248 in Ep 7, LR: [8.8e-05, 8.8e-05] Time: 1.93s , train_loss: 8.0901, 
['0.0000']
2023-07-27_23:47:55, Step 1247/1248 in Ep 7, LR: [8.8e-05, 8.8e-05] Time: 1.95s , train_loss: 8.1049, 
['0.0000']
Epoch 7, LR 9.046039886902862e-05, Train_Time 2466.20s, Loss: 8.1049
['0.0000']
Evaluation on train set, train losses 8.104919836307184
 ma: 0.8789, label_f1: 0.7737, pos_recall: 0.7791 , neg_recall: 0.9786 
 Acc: 0.8834, Prec: 0.9271, Rec: 0.9437, F1: 0.9353
Evaluation on test set, valid losses 7.020078647613525
 ma: 0.8924, label_f1: 0.7947, pos_recall: 0.8037 , neg_recall: 0.9810 
 Acc: 0.9001, Prec: 0.9375, Rec: 0.9527, F1: 0.9451
2023-07-27_23:49:15
------------------------------------------------------------
2023-07-27_23:52:39, Step 99/1248 in Ep 8, LR: [8.7e-05, 8.7e-05] Time: 1.94s , train_loss: 7.1135, 
['0.0000']
2023-07-27_23:55:55, Step 199/1248 in Ep 8, LR: [8.7e-05, 8.7e-05] Time: 1.95s , train_loss: 7.1549, 
['0.0000']
2023-07-27_23:59:11, Step 299/1248 in Ep 8, LR: [8.7e-05, 8.7e-05] Time: 1.92s , train_loss: 7.1611, 
['0.0000']
2023-07-28_00:02:27, Step 399/1248 in Ep 8, LR: [8.7e-05, 8.7e-05] Time: 2.09s , train_loss: 7.1731, 
['0.0000']
2023-07-28_00:05:43, Step 499/1248 in Ep 8, LR: [8.6e-05, 8.6e-05] Time: 1.90s , train_loss: 7.1989, 
['0.0000']
2023-07-28_00:09:03, Step 599/1248 in Ep 8, LR: [8.6e-05, 8.6e-05] Time: 1.88s , train_loss: 7.2613, 
['0.0000']
2023-07-28_00:12:20, Step 699/1248 in Ep 8, LR: [8.6e-05, 8.6e-05] Time: 2.18s , train_loss: 7.3166, 
['0.0000']
2023-07-28_00:15:36, Step 799/1248 in Ep 8, LR: [8.6e-05, 8.6e-05] Time: 1.90s , train_loss: 7.3799, 
['0.0000']
2023-07-28_00:18:53, Step 899/1248 in Ep 8, LR: [8.5e-05, 8.5e-05] Time: 1.98s , train_loss: 7.4166, 
['0.0000']
2023-07-28_00:22:09, Step 999/1248 in Ep 8, LR: [8.5e-05, 8.5e-05] Time: 2.06s , train_loss: 7.4639, 
['0.0000']
2023-07-28_00:25:26, Step 1099/1248 in Ep 8, LR: [8.5e-05, 8.5e-05] Time: 2.09s , train_loss: 7.4825, 
['0.0000']
2023-07-28_00:28:43, Step 1199/1248 in Ep 8, LR: [8.5e-05, 8.5e-05] Time: 1.94s , train_loss: 7.5215, 
['0.0000']
2023-07-28_00:30:21, Step 1247/1248 in Ep 8, LR: [8.5e-05, 8.5e-05] Time: 1.96s , train_loss: 7.5371, 
['0.0000']
Epoch 8, LR 8.766591972688038e-05, Train_Time 2460.56s, Loss: 7.5371
['0.0000']
Evaluation on train set, train losses 7.537083830970984
 ma: 0.8839, label_f1: 0.7825, pos_recall: 0.7875 , neg_recall: 0.9804 
 Acc: 0.8918, Prec: 0.9326, Rec: 0.9481, F1: 0.9403
Evaluation on test set, valid losses 6.430326780319214
 ma: 0.8863, label_f1: 0.7900, pos_recall: 0.7892 , neg_recall: 0.9834 
 Acc: 0.9084, Prec: 0.9431, Rec: 0.9568, F1: 0.9499
2023-07-28_00:31:41
------------------------------------------------------------
2023-07-28_00:35:05, Step 99/1248 in Ep 9, LR: [8.4e-05, 8.4e-05] Time: 1.89s , train_loss: 6.3526, 
['0.0000']
2023-07-28_00:38:22, Step 199/1248 in Ep 9, LR: [8.4e-05, 8.4e-05] Time: 1.96s , train_loss: 6.5425, 
['0.0000']
2023-07-28_00:41:39, Step 299/1248 in Ep 9, LR: [8.4e-05, 8.4e-05] Time: 2.07s , train_loss: 6.6975, 
['0.0000']
2023-07-28_00:44:55, Step 399/1248 in Ep 9, LR: [8.4e-05, 8.4e-05] Time: 1.95s , train_loss: 6.7636, 
['0.0000']
2023-07-28_00:48:12, Step 499/1248 in Ep 9, LR: [8.3e-05, 8.3e-05] Time: 1.93s , train_loss: 6.8106, 
['0.0000']
2023-07-28_00:51:28, Step 599/1248 in Ep 9, LR: [8.3e-05, 8.3e-05] Time: 1.98s , train_loss: 6.8450, 
['0.0000']
2023-07-28_00:54:48, Step 699/1248 in Ep 9, LR: [8.3e-05, 8.3e-05] Time: 1.91s , train_loss: 6.8918, 
['0.0000']
2023-07-28_00:58:05, Step 799/1248 in Ep 9, LR: [8.2e-05, 8.2e-05] Time: 1.91s , train_loss: 6.9276, 
['0.0000']
2023-07-28_01:01:21, Step 899/1248 in Ep 9, LR: [8.2e-05, 8.2e-05] Time: 1.92s , train_loss: 6.9559, 
['0.0000']
2023-07-28_01:04:38, Step 999/1248 in Ep 9, LR: [8.2e-05, 8.2e-05] Time: 2.07s , train_loss: 6.9883, 
['0.0000']
2023-07-28_01:07:54, Step 1099/1248 in Ep 9, LR: [8.2e-05, 8.2e-05] Time: 1.92s , train_loss: 7.0257, 
['0.0000']
2023-07-28_01:11:11, Step 1199/1248 in Ep 9, LR: [8.1e-05, 8.1e-05] Time: 1.97s , train_loss: 7.0497, 
['0.0000']
2023-07-28_01:12:45, Step 1247/1248 in Ep 9, LR: [8.1e-05, 8.1e-05] Time: 1.93s , train_loss: 7.0683, 
['0.0000']
Epoch 9, LR 8.45685793168939e-05, Train_Time 2458.54s, Loss: 7.0683
['0.0000']
Evaluation on train set, train losses 7.068255911270778
 ma: 0.8877, label_f1: 0.7892, pos_recall: 0.7939 , neg_recall: 0.9816 
 Acc: 0.8985, Prec: 0.9369, Rec: 0.9514, F1: 0.9441
Evaluation on test set, valid losses 6.2362784481048585
 ma: 0.8828, label_f1: 0.7885, pos_recall: 0.7815 , neg_recall: 0.9842 
 Acc: 0.9116, Prec: 0.9455, Rec: 0.9580, F1: 0.9517
2023-07-28_01:14:05
------------------------------------------------------------
2023-07-28_01:17:33, Step 99/1248 in Ep 10, LR: [8.1e-05, 8.1e-05] Time: 1.90s , train_loss: 6.0663, 
['0.0000']
2023-07-28_01:20:49, Step 199/1248 in Ep 10, LR: [8.1e-05, 8.1e-05] Time: 2.12s , train_loss: 6.0948, 
['0.0000']
2023-07-28_01:24:05, Step 299/1248 in Ep 10, LR: [8.0e-05, 8.0e-05] Time: 1.98s , train_loss: 6.1079, 
['0.0000']
2023-07-28_01:27:22, Step 399/1248 in Ep 10, LR: [8.0e-05, 8.0e-05] Time: 1.94s , train_loss: 6.1959, 
['0.0000']
2023-07-28_01:30:39, Step 499/1248 in Ep 10, LR: [8.0e-05, 8.0e-05] Time: 1.92s , train_loss: 6.2748, 
['0.0000']
2023-07-28_01:33:56, Step 599/1248 in Ep 10, LR: [7.9e-05, 7.9e-05] Time: 2.13s , train_loss: 6.2911, 
['0.0000']
2023-07-28_01:37:12, Step 699/1248 in Ep 10, LR: [7.9e-05, 7.9e-05] Time: 1.93s , train_loss: 6.3301, 
['0.0000']
2023-07-28_01:40:33, Step 799/1248 in Ep 10, LR: [7.9e-05, 7.9e-05] Time: 1.91s , train_loss: 6.3758, 
['0.0000']
2023-07-28_01:43:50, Step 899/1248 in Ep 10, LR: [7.9e-05, 7.9e-05] Time: 2.07s , train_loss: 6.4166, 
['0.0000']
2023-07-28_01:47:07, Step 999/1248 in Ep 10, LR: [7.8e-05, 7.8e-05] Time: 1.93s , train_loss: 6.4642, 
['0.0000']
2023-07-28_01:50:24, Step 1099/1248 in Ep 10, LR: [7.8e-05, 7.8e-05] Time: 1.90s , train_loss: 6.5033, 
['0.0000']
2023-07-28_01:53:41, Step 1199/1248 in Ep 10, LR: [7.8e-05, 7.8e-05] Time: 1.88s , train_loss: 6.5484, 
['0.0000']
2023-07-28_01:55:15, Step 1247/1248 in Ep 10, LR: [7.8e-05, 7.8e-05] Time: 1.93s , train_loss: 6.5683, 
['0.0000']
Epoch 10, LR 8.119331560284375e-05, Train_Time 2464.18s, Loss: 6.5683
['0.0000']
Evaluation on train set, train losses 6.568266743459763
 ma: 0.8936, label_f1: 0.8005, pos_recall: 0.8043 , neg_recall: 0.9829 
 Acc: 0.9055, Prec: 0.9414, Rec: 0.9550, F1: 0.9482
Evaluation on test set, valid losses 5.699802431106567
 ma: 0.8977, label_f1: 0.8040, pos_recall: 0.8101 , neg_recall: 0.9853 
 Acc: 0.9201, Prec: 0.9504, Rec: 0.9628, F1: 0.9565
2023-07-28_01:56:34
------------------------------------------------------------
2023-07-28_02:00:00, Step 99/1248 in Ep 11, LR: [7.7e-05, 7.7e-05] Time: 1.90s , train_loss: 5.5564, 
['0.0000']
2023-07-28_02:03:19, Step 199/1248 in Ep 11, LR: [7.7e-05, 7.7e-05] Time: 1.92s , train_loss: 5.6783, 
['0.0000']
2023-07-28_02:06:36, Step 299/1248 in Ep 11, LR: [7.7e-05, 7.7e-05] Time: 1.91s , train_loss: 5.7260, 
['0.0000']
2023-07-28_02:09:53, Step 399/1248 in Ep 11, LR: [7.6e-05, 7.6e-05] Time: 1.96s , train_loss: 5.7916, 
['0.0000']
2023-07-28_02:13:10, Step 499/1248 in Ep 11, LR: [7.6e-05, 7.6e-05] Time: 2.14s , train_loss: 5.8398, 
['0.0000']
2023-07-28_02:16:26, Step 599/1248 in Ep 11, LR: [7.6e-05, 7.6e-05] Time: 1.91s , train_loss: 5.8671, 
['0.0000']
2023-07-28_02:19:44, Step 699/1248 in Ep 11, LR: [7.5e-05, 7.5e-05] Time: 1.95s , train_loss: 5.9118, 
['0.0000']
2023-07-28_02:23:01, Step 799/1248 in Ep 11, LR: [7.5e-05, 7.5e-05] Time: 1.90s , train_loss: 5.9731, 
['0.0000']
2023-07-28_02:26:23, Step 899/1248 in Ep 11, LR: [7.5e-05, 7.5e-05] Time: 1.87s , train_loss: 6.0136, 
['0.0000']
2023-07-28_02:29:41, Step 999/1248 in Ep 11, LR: [7.5e-05, 7.5e-05] Time: 1.94s , train_loss: 6.0404, 
['0.0000']
2023-07-28_02:32:58, Step 1099/1248 in Ep 11, LR: [7.4e-05, 7.4e-05] Time: 1.91s , train_loss: 6.0755, 
['0.0000']
2023-07-28_02:36:16, Step 1199/1248 in Ep 11, LR: [7.4e-05, 7.4e-05] Time: 2.06s , train_loss: 6.1198, 
['0.0000']
2023-07-28_02:37:50, Step 1247/1248 in Ep 11, LR: [7.4e-05, 7.4e-05] Time: 2.08s , train_loss: 6.1299, 
['0.0000']
Epoch 11, LR 7.756730422353253e-05, Train_Time 2469.46s, Loss: 6.1299
['0.0000']
Evaluation on train set, train losses 6.129879651161341
 ma: 0.8958, label_f1: 0.8043, pos_recall: 0.8075 , neg_recall: 0.9841 
 Acc: 0.9119, Prec: 0.9456, Rec: 0.9582, F1: 0.9519
Evaluation on test set, valid losses 5.199458579063416
 ma: 0.9003, label_f1: 0.8017, pos_recall: 0.8140 , neg_recall: 0.9867 
 Acc: 0.9259, Prec: 0.9552, Rec: 0.9646, F1: 0.9599
2023-07-28_02:39:10
------------------------------------------------------------
2023-07-28_02:42:37, Step 99/1248 in Ep 12, LR: [7.3e-05, 7.3e-05] Time: 1.92s , train_loss: 5.3198, 
['0.0000']
2023-07-28_02:45:53, Step 199/1248 in Ep 12, LR: [7.3e-05, 7.3e-05] Time: 1.94s , train_loss: 5.3655, 
['0.0000']
2023-07-28_02:49:14, Step 299/1248 in Ep 12, LR: [7.3e-05, 7.3e-05] Time: 2.16s , train_loss: 5.4133, 
['0.0000']
2023-07-28_02:52:30, Step 399/1248 in Ep 12, LR: [7.2e-05, 7.2e-05] Time: 1.91s , train_loss: 5.4545, 
['0.0000']
2023-07-28_02:55:47, Step 499/1248 in Ep 12, LR: [7.2e-05, 7.2e-05] Time: 1.96s , train_loss: 5.4754, 
['0.0000']
2023-07-28_02:59:05, Step 599/1248 in Ep 12, LR: [7.2e-05, 7.2e-05] Time: 1.94s , train_loss: 5.5050, 
['0.0000']
2023-07-28_03:02:21, Step 699/1248 in Ep 12, LR: [7.1e-05, 7.1e-05] Time: 1.92s , train_loss: 5.5425, 
['0.0000']
2023-07-28_03:05:39, Step 799/1248 in Ep 12, LR: [7.1e-05, 7.1e-05] Time: 1.93s , train_loss: 5.5882, 
['0.0000']
2023-07-28_03:08:56, Step 899/1248 in Ep 12, LR: [7.1e-05, 7.1e-05] Time: 1.98s , train_loss: 5.6198, 
['0.0000']
2023-07-28_03:12:17, Step 999/1248 in Ep 12, LR: [7.0e-05, 7.0e-05] Time: 2.16s , train_loss: 5.6577, 
['0.0000']
2023-07-28_03:15:34, Step 1099/1248 in Ep 12, LR: [7.0e-05, 7.0e-05] Time: 1.96s , train_loss: 5.6793, 
['0.0000']
2023-07-28_03:18:51, Step 1199/1248 in Ep 12, LR: [7.0e-05, 7.0e-05] Time: 1.94s , train_loss: 5.7077, 
['0.0000']
2023-07-28_03:20:25, Step 1247/1248 in Ep 12, LR: [7.0e-05, 7.0e-05] Time: 1.90s , train_loss: 5.7235, 
['0.0000']
Epoch 12, LR 7.371973969052628e-05, Train_Time 2467.76s, Loss: 5.7235
['0.0000']
Evaluation on train set, train losses 5.723524872691203
 ma: 0.8990, label_f1: 0.8101, pos_recall: 0.8127 , neg_recall: 0.9853 
 Acc: 0.9179, Prec: 0.9494, Rec: 0.9612, F1: 0.9553
Evaluation on test set, valid losses 4.687478079795837
 ma: 0.9023, label_f1: 0.8177, pos_recall: 0.8162 , neg_recall: 0.9884 
 Acc: 0.9342, Prec: 0.9596, Rec: 0.9693, F1: 0.9644
2023-07-28_03:21:45
------------------------------------------------------------
2023-07-28_03:25:12, Step 99/1248 in Ep 13, LR: [6.9e-05, 6.9e-05] Time: 1.86s , train_loss: 4.8683, 
['0.0000']
2023-07-28_03:28:25, Step 199/1248 in Ep 13, LR: [6.9e-05, 6.9e-05] Time: 1.88s , train_loss: 4.8943, 
['0.0000']
2023-07-28_03:31:37, Step 299/1248 in Ep 13, LR: [6.9e-05, 6.9e-05] Time: 2.19s , train_loss: 4.9401, 
['0.0000']
2023-07-28_03:34:52, Step 399/1248 in Ep 13, LR: [6.8e-05, 6.8e-05] Time: 1.84s , train_loss: 4.9974, 
['0.0000']
2023-07-28_03:38:03, Step 499/1248 in Ep 13, LR: [6.8e-05, 6.8e-05] Time: 1.87s , train_loss: 5.0656, 
['0.0000']
2023-07-28_03:41:13, Step 599/1248 in Ep 13, LR: [6.8e-05, 6.8e-05] Time: 1.90s , train_loss: 5.0928, 
['0.0000']
2023-07-28_03:44:25, Step 699/1248 in Ep 13, LR: [6.7e-05, 6.7e-05] Time: 2.08s , train_loss: 5.1236, 
['0.0000']
2023-07-28_03:47:36, Step 799/1248 in Ep 13, LR: [6.7e-05, 6.7e-05] Time: 1.88s , train_loss: 5.1519, 
['0.0000']
2023-07-28_03:50:47, Step 899/1248 in Ep 13, LR: [6.7e-05, 6.7e-05] Time: 1.88s , train_loss: 5.1799, 
['0.0000']
2023-07-28_03:53:58, Step 999/1248 in Ep 13, LR: [6.6e-05, 6.6e-05] Time: 1.84s , train_loss: 5.2160, 
['0.0000']
2023-07-28_03:57:09, Step 1099/1248 in Ep 13, LR: [6.6e-05, 6.6e-05] Time: 1.86s , train_loss: 5.2460, 
['0.0000']
2023-07-28_04:00:20, Step 1199/1248 in Ep 13, LR: [6.6e-05, 6.6e-05] Time: 1.99s , train_loss: 5.2659, 
['0.0000']
2023-07-28_04:01:51, Step 1247/1248 in Ep 13, LR: [6.5e-05, 6.5e-05] Time: 2.10s , train_loss: 5.2764, 
['0.0000']
Epoch 13, LR 6.968160033111349e-05, Train_Time 2398.86s, Loss: 5.2764
['0.0000']
Evaluation on train set, train losses 5.276389743655156
 ma: 0.9040, label_f1: 0.8189, pos_recall: 0.8215 , neg_recall: 0.9865 
 Acc: 0.9244, Prec: 0.9536, Rec: 0.9643, F1: 0.9589
Evaluation on test set, valid losses 4.384793433189392
 ma: 0.9101, label_f1: 0.8293, pos_recall: 0.8309 , neg_recall: 0.9893 
 Acc: 0.9386, Prec: 0.9631, Rec: 0.9708, F1: 0.9669
2023-07-28_04:03:08
------------------------------------------------------------
2023-07-28_04:06:28, Step 99/1248 in Ep 14, LR: [6.5e-05, 6.5e-05] Time: 1.88s , train_loss: 4.3486, 
['0.0000']
2023-07-28_04:09:37, Step 199/1248 in Ep 14, LR: [6.5e-05, 6.5e-05] Time: 1.87s , train_loss: 4.4292, 
['0.0000']
2023-07-28_04:12:47, Step 299/1248 in Ep 14, LR: [6.4e-05, 6.4e-05] Time: 1.85s , train_loss: 4.4750, 
['0.0000']
2023-07-28_04:15:57, Step 399/1248 in Ep 14, LR: [6.4e-05, 6.4e-05] Time: 1.86s , train_loss: 4.5422, 
['0.0000']
2023-07-28_04:19:08, Step 499/1248 in Ep 14, LR: [6.4e-05, 6.4e-05] Time: 2.01s , train_loss: 4.6056, 
['0.0000']
2023-07-28_04:22:17, Step 599/1248 in Ep 14, LR: [6.3e-05, 6.3e-05] Time: 1.84s , train_loss: 4.6499, 
['0.0000']
2023-07-28_04:25:27, Step 699/1248 in Ep 14, LR: [6.3e-05, 6.3e-05] Time: 1.89s , train_loss: 4.6874, 
['0.0000']
2023-07-28_04:28:37, Step 799/1248 in Ep 14, LR: [6.3e-05, 6.3e-05] Time: 1.86s , train_loss: 4.7182, 
['0.0000']
2023-07-28_04:31:47, Step 899/1248 in Ep 14, LR: [6.2e-05, 6.2e-05] Time: 1.85s , train_loss: 4.7478, 
['0.0000']
2023-07-28_04:34:57, Step 999/1248 in Ep 14, LR: [6.2e-05, 6.2e-05] Time: 2.00s , train_loss: 4.7773, 
['0.0000']
2023-07-28_04:38:11, Step 1099/1248 in Ep 14, LR: [6.2e-05, 6.2e-05] Time: 1.90s , train_loss: 4.8054, 
['0.0000']
2023-07-28_04:41:24, Step 1199/1248 in Ep 14, LR: [6.1e-05, 6.1e-05] Time: 1.85s , train_loss: 4.8311, 
['0.0000']
2023-07-28_04:42:55, Step 1247/1248 in Ep 14, LR: [6.1e-05, 6.1e-05] Time: 1.87s , train_loss: 4.8459, 
['0.0000']
Epoch 14, LR 6.548539886902863e-05, Train_Time 2379.42s, Loss: 4.8459
['0.0000']
Evaluation on train set, train losses 4.845867993548895
 ma: 0.9071, label_f1: 0.8247, pos_recall: 0.8266 , neg_recall: 0.9877 
 Acc: 0.9304, Prec: 0.9573, Rec: 0.9674, F1: 0.9623
Evaluation on test set, valid losses 4.027869815826416
 ma: 0.9153, label_f1: 0.8388, pos_recall: 0.8406 , neg_recall: 0.9900 
 Acc: 0.9421, Prec: 0.9645, Rec: 0.9732, F1: 0.9688
2023-07-28_04:44:11
------------------------------------------------------------
2023-07-28_04:47:31, Step 99/1248 in Ep 15, LR: [6.1e-05, 6.1e-05] Time: 1.98s , train_loss: 4.0379, 
['0.0000']
2023-07-28_04:50:42, Step 199/1248 in Ep 15, LR: [6.0e-05, 6.0e-05] Time: 1.86s , train_loss: 4.1124, 
['0.0000']
2023-07-28_04:53:52, Step 299/1248 in Ep 15, LR: [6.0e-05, 6.0e-05] Time: 2.03s , train_loss: 4.1400, 
['0.0000']
2023-07-28_04:57:02, Step 399/1248 in Ep 15, LR: [6.0e-05, 6.0e-05] Time: 1.90s , train_loss: 4.1776, 
['0.0000']
2023-07-28_05:00:13, Step 499/1248 in Ep 15, LR: [5.9e-05, 5.9e-05] Time: 1.90s , train_loss: 4.2192, 
['0.0000']
2023-07-28_05:03:23, Step 599/1248 in Ep 15, LR: [5.9e-05, 5.9e-05] Time: 1.88s , train_loss: 4.2664, 
['0.0000']
2023-07-28_05:06:33, Step 699/1248 in Ep 15, LR: [5.9e-05, 5.9e-05] Time: 1.88s , train_loss: 4.2980, 
['0.0000']
2023-07-28_05:09:44, Step 799/1248 in Ep 15, LR: [5.8e-05, 5.8e-05] Time: 2.10s , train_loss: 4.3387, 
['0.0000']
2023-07-28_05:12:57, Step 899/1248 in Ep 15, LR: [5.8e-05, 5.8e-05] Time: 1.86s , train_loss: 4.3587, 
['0.0000']
2023-07-28_05:16:07, Step 999/1248 in Ep 15, LR: [5.8e-05, 5.8e-05] Time: 1.90s , train_loss: 4.3867, 
['0.0000']
2023-07-28_05:19:20, Step 1099/1248 in Ep 15, LR: [5.7e-05, 5.7e-05] Time: 1.94s , train_loss: 4.4085, 
['0.0000']
2023-07-28_05:22:33, Step 1199/1248 in Ep 15, LR: [5.7e-05, 5.7e-05] Time: 1.96s , train_loss: 4.4283, 
['0.0000']
2023-07-28_05:24:05, Step 1247/1248 in Ep 15, LR: [5.7e-05, 5.7e-05] Time: 2.00s , train_loss: 4.4378, 
['0.0000']
Epoch 15, LR 6.116492065111791e-05, Train_Time 2385.88s, Loss: 4.4378
['0.0000']
Evaluation on train set, train losses 4.437797095913154
 ma: 0.9115, label_f1: 0.8334, pos_recall: 0.8341 , neg_recall: 0.9888 
 Acc: 0.9365, Prec: 0.9611, Rec: 0.9704, F1: 0.9657
Evaluation on test set, valid losses 3.679175597190857
 ma: 0.9132, label_f1: 0.8360, pos_recall: 0.8361 , neg_recall: 0.9903 
 Acc: 0.9482, Prec: 0.9678, Rec: 0.9767, F1: 0.9722
2023-07-28_05:25:22
------------------------------------------------------------
2023-07-28_05:28:44, Step 99/1248 in Ep 16, LR: [5.6e-05, 5.6e-05] Time: 1.88s , train_loss: 3.7089, 
['0.0000']
2023-07-28_05:31:54, Step 199/1248 in Ep 16, LR: [5.6e-05, 5.6e-05] Time: 2.02s , train_loss: 3.7499, 
['0.0000']
2023-07-28_05:35:05, Step 299/1248 in Ep 16, LR: [5.6e-05, 5.6e-05] Time: 1.87s , train_loss: 3.8278, 
['0.0000']
2023-07-28_05:38:16, Step 399/1248 in Ep 16, LR: [5.5e-05, 5.5e-05] Time: 1.85s , train_loss: 3.8429, 
['0.0000']
2023-07-28_05:41:26, Step 499/1248 in Ep 16, LR: [5.5e-05, 5.5e-05] Time: 1.91s , train_loss: 3.8472, 
['0.0000']
2023-07-28_05:44:38, Step 599/1248 in Ep 16, LR: [5.5e-05, 5.5e-05] Time: 1.92s , train_loss: 3.8846, 
['0.0000']
2023-07-28_05:47:50, Step 699/1248 in Ep 16, LR: [5.4e-05, 5.4e-05] Time: 2.05s , train_loss: 3.9246, 
['0.0000']
2023-07-28_05:50:59, Step 799/1248 in Ep 16, LR: [5.4e-05, 5.4e-05] Time: 1.86s , train_loss: 3.9468, 
['0.0000']
2023-07-28_05:54:09, Step 899/1248 in Ep 16, LR: [5.4e-05, 5.4e-05] Time: 1.87s , train_loss: 3.9691, 
['0.0000']
2023-07-28_05:57:20, Step 999/1248 in Ep 16, LR: [5.3e-05, 5.3e-05] Time: 1.89s , train_loss: 3.9962, 
['0.0000']
2023-07-28_06:00:32, Step 1099/1248 in Ep 16, LR: [5.3e-05, 5.3e-05] Time: 2.07s , train_loss: 4.0208, 
['0.0000']
2023-07-28_06:03:42, Step 1199/1248 in Ep 16, LR: [5.2e-05, 5.2e-05] Time: 1.87s , train_loss: 4.0324, 
['0.0000']
2023-07-28_06:05:13, Step 1247/1248 in Ep 16, LR: [5.2e-05, 5.2e-05] Time: 1.97s , train_loss: 4.0354, 
['0.0000']
Epoch 16, LR 5.67549516275919e-05, Train_Time 2382.01s, Loss: 4.0354
['0.0000']
Evaluation on train set, train losses 4.035400398075581
 ma: 0.9145, label_f1: 0.8385, pos_recall: 0.8391 , neg_recall: 0.9898 
 Acc: 0.9422, Prec: 0.9647, Rec: 0.9731, F1: 0.9689
Evaluation on test set, valid losses 3.5128906106948854
 ma: 0.9232, label_f1: 0.8471, pos_recall: 0.8552 , neg_recall: 0.9912 
 Acc: 0.9496, Prec: 0.9690, Rec: 0.9772, F1: 0.9731
2023-07-28_06:06:28
------------------------------------------------------------
2023-07-28_06:09:49, Step 99/1248 in Ep 17, LR: [5.2e-05, 5.2e-05] Time: 1.89s , train_loss: 3.4483, 
['0.0000']
2023-07-28_06:12:59, Step 199/1248 in Ep 17, LR: [5.2e-05, 5.2e-05] Time: 1.88s , train_loss: 3.4634, 
['0.0000']
2023-07-28_06:16:09, Step 299/1248 in Ep 17, LR: [5.1e-05, 5.1e-05] Time: 1.88s , train_loss: 3.4931, 
['0.0000']
2023-07-28_06:19:19, Step 399/1248 in Ep 17, LR: [5.1e-05, 5.1e-05] Time: 1.84s , train_loss: 3.5107, 
['0.0000']
2023-07-28_06:22:28, Step 499/1248 in Ep 17, LR: [5.0e-05, 5.0e-05] Time: 2.00s , train_loss: 3.5333, 
['0.0000']
2023-07-28_06:25:39, Step 599/1248 in Ep 17, LR: [5.0e-05, 5.0e-05] Time: 1.85s , train_loss: 3.5463, 
['0.0000']
2023-07-28_06:28:48, Step 699/1248 in Ep 17, LR: [5.0e-05, 5.0e-05] Time: 1.85s , train_loss: 3.5658, 
['0.0000']
2023-07-28_06:32:00, Step 799/1248 in Ep 17, LR: [4.9e-05, 4.9e-05] Time: 1.89s , train_loss: 3.5922, 
['0.0000']
2023-07-28_06:35:10, Step 899/1248 in Ep 17, LR: [4.9e-05, 4.9e-05] Time: 2.20s , train_loss: 3.6032, 
['0.0000']
2023-07-28_06:38:21, Step 999/1248 in Ep 17, LR: [4.9e-05, 4.9e-05] Time: 1.85s , train_loss: 3.6166, 
['0.0000']
2023-07-28_06:41:31, Step 1099/1248 in Ep 17, LR: [4.8e-05, 4.8e-05] Time: 2.01s , train_loss: 3.6335, 
['0.0000']
2023-07-28_06:44:41, Step 1199/1248 in Ep 17, LR: [4.8e-05, 4.8e-05] Time: 1.89s , train_loss: 3.6516, 
['0.0000']
2023-07-28_06:46:12, Step 1247/1248 in Ep 17, LR: [4.8e-05, 4.8e-05] Time: 1.87s , train_loss: 3.6553, 
['0.0000']
Epoch 17, LR 5.2290998276008226e-05, Train_Time 2374.05s, Loss: 3.6553
['0.0000']
Evaluation on train set, train losses 3.6553327354292073
 ma: 0.9175, label_f1: 0.8440, pos_recall: 0.8441 , neg_recall: 0.9909 
 Acc: 0.9478, Prec: 0.9682, Rec: 0.9758, F1: 0.9720
Evaluation on test set, valid losses 3.0659377121925355
 ma: 0.9238, label_f1: 0.8534, pos_recall: 0.8554 , neg_recall: 0.9922 
 Acc: 0.9566, Prec: 0.9731, Rec: 0.9806, F1: 0.9768
2023-07-28_06:47:29
------------------------------------------------------------
2023-07-28_06:50:53, Step 99/1248 in Ep 18, LR: [4.7e-05, 4.7e-05] Time: 1.87s , train_loss: 2.9813, 
['0.0000']
2023-07-28_06:54:03, Step 199/1248 in Ep 18, LR: [4.7e-05, 4.7e-05] Time: 1.84s , train_loss: 3.0357, 
['0.0000']
2023-07-28_06:57:12, Step 299/1248 in Ep 18, LR: [4.7e-05, 4.7e-05] Time: 2.02s , train_loss: 3.0910, 
['0.0000']
2023-07-28_07:00:23, Step 399/1248 in Ep 18, LR: [4.6e-05, 4.6e-05] Time: 1.85s , train_loss: 3.1153, 
['0.0000']
2023-07-28_07:03:33, Step 499/1248 in Ep 18, LR: [4.6e-05, 4.6e-05] Time: 1.90s , train_loss: 3.1356, 
['0.0000']
2023-07-28_07:06:43, Step 599/1248 in Ep 18, LR: [4.6e-05, 4.6e-05] Time: 1.92s , train_loss: 3.1535, 
['0.0000']
2023-07-28_07:09:53, Step 699/1248 in Ep 18, LR: [4.5e-05, 4.5e-05] Time: 2.03s , train_loss: 3.1746, 
['0.0000']
2023-07-28_07:13:06, Step 799/1248 in Ep 18, LR: [4.5e-05, 4.5e-05] Time: 1.87s , train_loss: 3.1893, 
['0.0000']
2023-07-28_07:16:16, Step 899/1248 in Ep 18, LR: [4.5e-05, 4.5e-05] Time: 1.84s , train_loss: 3.2100, 
['0.0000']
2023-07-28_07:19:26, Step 999/1248 in Ep 18, LR: [4.4e-05, 4.4e-05] Time: 1.88s , train_loss: 3.2246, 
['0.0000']
2023-07-28_07:22:35, Step 1099/1248 in Ep 18, LR: [4.4e-05, 4.4e-05] Time: 1.86s , train_loss: 3.2324, 
['0.0000']
2023-07-28_07:25:45, Step 1199/1248 in Ep 18, LR: [4.4e-05, 4.4e-05] Time: 1.98s , train_loss: 3.2562, 
['0.0000']
2023-07-28_07:27:17, Step 1247/1248 in Ep 18, LR: [4.3e-05, 4.3e-05] Time: 2.02s , train_loss: 3.2678, 
['0.0000']
Epoch 18, LR 4.780900172399179e-05, Train_Time 2376.97s, Loss: 3.2678
['0.0000']
Evaluation on train set, train losses 3.267847514114319
 ma: 0.9232, label_f1: 0.8529, pos_recall: 0.8546 , neg_recall: 0.9918 
 Acc: 0.9533, Prec: 0.9716, Rec: 0.9785, F1: 0.9750
Evaluation on test set, valid losses 2.895605019569397
 ma: 0.9268, label_f1: 0.8569, pos_recall: 0.8607 , neg_recall: 0.9930 
 Acc: 0.9583, Prec: 0.9746, Rec: 0.9809, F1: 0.9778
2023-07-28_07:28:35
------------------------------------------------------------
2023-07-28_07:32:00, Step 99/1248 in Ep 19, LR: [4.3e-05, 4.3e-05] Time: 2.03s , train_loss: 2.7265, 
['0.0000']
2023-07-28_07:35:11, Step 199/1248 in Ep 19, LR: [4.3e-05, 4.3e-05] Time: 1.89s , train_loss: 2.7284, 
['0.0000']
2023-07-28_07:38:22, Step 299/1248 in Ep 19, LR: [4.2e-05, 4.2e-05] Time: 2.13s , train_loss: 2.7654, 
['0.0000']
2023-07-28_07:41:33, Step 399/1248 in Ep 19, LR: [4.2e-05, 4.2e-05] Time: 1.84s , train_loss: 2.7651, 
['0.0000']
2023-07-28_07:44:42, Step 499/1248 in Ep 19, LR: [4.2e-05, 4.2e-05] Time: 1.86s , train_loss: 2.8003, 
['0.0000']
2023-07-28_07:47:52, Step 599/1248 in Ep 19, LR: [4.1e-05, 4.1e-05] Time: 2.02s , train_loss: 2.8148, 
['0.0000']
2023-07-28_07:51:01, Step 699/1248 in Ep 19, LR: [4.1e-05, 4.1e-05] Time: 1.84s , train_loss: 2.8262, 
['0.0000']
2023-07-28_07:54:11, Step 799/1248 in Ep 19, LR: [4.1e-05, 4.1e-05] Time: 1.85s , train_loss: 2.8502, 
['0.0000']
2023-07-28_07:57:21, Step 899/1248 in Ep 19, LR: [4.0e-05, 4.0e-05] Time: 1.89s , train_loss: 2.8678, 
['0.0000']
2023-07-28_08:00:33, Step 999/1248 in Ep 19, LR: [4.0e-05, 4.0e-05] Time: 2.10s , train_loss: 2.8802, 
['0.0000']
2023-07-28_08:03:45, Step 1099/1248 in Ep 19, LR: [3.9e-05, 3.9e-05] Time: 1.99s , train_loss: 2.8942, 
['0.0000']
2023-07-28_08:06:56, Step 1199/1248 in Ep 19, LR: [3.9e-05, 3.9e-05] Time: 1.87s , train_loss: 2.9056, 
['0.0000']
2023-07-28_08:08:28, Step 1247/1248 in Ep 19, LR: [3.9e-05, 3.9e-05] Time: 1.84s , train_loss: 2.9061, 
['0.0000']
Epoch 19, LR 4.334504837240811e-05, Train_Time 2381.70s, Loss: 2.9061
['0.0000']
Evaluation on train set, train losses 2.9060938556989035
 ma: 0.9249, label_f1: 0.8569, pos_recall: 0.8569 , neg_recall: 0.9928 
 Acc: 0.9587, Prec: 0.9749, Rec: 0.9811, F1: 0.9780
Evaluation on test set, valid losses 2.4894909586906433
 ma: 0.9268, label_f1: 0.8610, pos_recall: 0.8598 , neg_recall: 0.9938 
 Acc: 0.9649, Prec: 0.9791, Rec: 0.9837, F1: 0.9814
2023-07-28_08:09:45
------------------------------------------------------------
2023-07-28_08:13:11, Step 99/1248 in Ep 20, LR: [3.9e-05, 3.9e-05] Time: 2.08s , train_loss: 2.3510, 
['0.0000']
2023-07-28_08:16:22, Step 199/1248 in Ep 20, LR: [3.8e-05, 3.8e-05] Time: 1.87s , train_loss: 2.3783, 
['0.0000']
2023-07-28_08:19:33, Step 299/1248 in Ep 20, LR: [3.8e-05, 3.8e-05] Time: 2.02s , train_loss: 2.4220, 
['0.0000']
2023-07-28_08:22:43, Step 399/1248 in Ep 20, LR: [3.8e-05, 3.8e-05] Time: 1.87s , train_loss: 2.4331, 
['0.0000']
2023-07-28_08:25:53, Step 499/1248 in Ep 20, LR: [3.7e-05, 3.7e-05] Time: 1.83s , train_loss: 2.4495, 
['0.0000']
2023-07-28_08:29:05, Step 599/1248 in Ep 20, LR: [3.7e-05, 3.7e-05] Time: 1.88s , train_loss: 2.4680, 
['0.0000']
2023-07-28_08:32:16, Step 699/1248 in Ep 20, LR: [3.6e-05, 3.6e-05] Time: 1.85s , train_loss: 2.4808, 
['0.0000']
2023-07-28_08:35:29, Step 799/1248 in Ep 20, LR: [3.6e-05, 3.6e-05] Time: 1.84s , train_loss: 2.4924, 
['0.0000']
2023-07-28_08:38:41, Step 899/1248 in Ep 20, LR: [3.6e-05, 3.6e-05] Time: 1.87s , train_loss: 2.5153, 
['0.0000']
2023-07-28_08:41:53, Step 999/1248 in Ep 20, LR: [3.5e-05, 3.5e-05] Time: 2.01s , train_loss: 2.5287, 
['0.0000']
2023-07-28_08:45:05, Step 1099/1248 in Ep 20, LR: [3.5e-05, 3.5e-05] Time: 1.89s , train_loss: 2.5422, 
['0.0000']
2023-07-28_08:48:16, Step 1199/1248 in Ep 20, LR: [3.5e-05, 3.5e-05] Time: 2.06s , train_loss: 2.5498, 
['0.0000']
2023-07-28_08:49:47, Step 1247/1248 in Ep 20, LR: [3.5e-05, 3.5e-05] Time: 2.02s , train_loss: 2.5546, 
['0.0000']
Epoch 20, LR 3.8935079348882105e-05, Train_Time 2391.16s, Loss: 2.5546
['0.0000']
Evaluation on train set, train losses 2.55463326177918
 ma: 0.9299, label_f1: 0.8656, pos_recall: 0.8661 , neg_recall: 0.9937 
 Acc: 0.9639, Prec: 0.9781, Rec: 0.9836, F1: 0.9809
Evaluation on test set, valid losses 2.113950210571289
 ma: 0.9344, label_f1: 0.8723, pos_recall: 0.8740 , neg_recall: 0.9949 
 Acc: 0.9702, Prec: 0.9820, Rec: 0.9864, F1: 0.9842
2023-07-28_08:51:05
------------------------------------------------------------
2023-07-28_08:54:29, Step 99/1248 in Ep 21, LR: [3.4e-05, 3.4e-05] Time: 1.82s , train_loss: 2.0468, 
['0.0000']
2023-07-28_08:57:43, Step 199/1248 in Ep 21, LR: [3.4e-05, 3.4e-05] Time: 1.89s , train_loss: 2.1286, 
['0.0000']
2023-07-28_09:01:00, Step 299/1248 in Ep 21, LR: [3.4e-05, 3.4e-05] Time: 1.90s , train_loss: 2.1355, 
['0.0000']
2023-07-28_09:04:35, Step 399/1248 in Ep 21, LR: [3.3e-05, 3.3e-05] Time: 1.96s , train_loss: 2.1491, 
['0.0000']
2023-07-28_09:07:50, Step 499/1248 in Ep 21, LR: [3.3e-05, 3.3e-05] Time: 1.86s , train_loss: 2.1488, 
['0.0000']
2023-07-28_09:11:07, Step 599/1248 in Ep 21, LR: [3.3e-05, 3.3e-05] Time: 1.91s , train_loss: 2.1495, 
['0.0000']
2023-07-28_09:14:22, Step 699/1248 in Ep 21, LR: [3.2e-05, 3.2e-05] Time: 1.88s , train_loss: 2.1567, 
['0.0000']
2023-07-28_09:17:37, Step 799/1248 in Ep 21, LR: [3.2e-05, 3.2e-05] Time: 1.90s , train_loss: 2.1607, 
['0.0000']
2023-07-28_09:20:58, Step 899/1248 in Ep 21, LR: [3.2e-05, 3.2e-05] Time: 1.90s , train_loss: 2.1730, 
['0.0000']
2023-07-28_09:24:21, Step 999/1248 in Ep 21, LR: [3.1e-05, 3.1e-05] Time: 1.98s , train_loss: 2.1819, 
['0.0000']
2023-07-28_09:27:43, Step 1099/1248 in Ep 21, LR: [3.1e-05, 3.1e-05] Time: 2.00s , train_loss: 2.1923, 
['0.0000']
2023-07-28_09:31:05, Step 1199/1248 in Ep 21, LR: [3.1e-05, 3.1e-05] Time: 2.31s , train_loss: 2.2030, 
['0.0000']
2023-07-28_09:32:42, Step 1247/1248 in Ep 21, LR: [3.0e-05, 3.0e-05] Time: 2.17s , train_loss: 2.2047, 
['0.0000']
Epoch 21, LR 3.461460113097139e-05, Train_Time 2485.55s, Loss: 2.2047
['0.0000']
Evaluation on train set, train losses 2.204658917796153
 ma: 0.9322, label_f1: 0.8700, pos_recall: 0.8698 , neg_recall: 0.9946 
 Acc: 0.9689, Prec: 0.9812, Rec: 0.9859, F1: 0.9835
Evaluation on test set, valid losses 1.8695493092536926
 ma: 0.9343, label_f1: 0.8769, pos_recall: 0.8731 , neg_recall: 0.9956 
 Acc: 0.9746, Prec: 0.9852, Rec: 0.9881, F1: 0.9866
2023-07-28_09:34:03
------------------------------------------------------------
2023-07-28_09:37:40, Step 99/1248 in Ep 22, LR: [3.0e-05, 3.0e-05] Time: 2.05s , train_loss: 1.7757, 
['0.0000']
2023-07-28_09:41:00, Step 199/1248 in Ep 22, LR: [3.0e-05, 3.0e-05] Time: 2.21s , train_loss: 1.7857, 
['0.0000']
2023-07-28_09:44:27, Step 299/1248 in Ep 22, LR: [2.9e-05, 2.9e-05] Time: 2.15s , train_loss: 1.8016, 
['0.0000']
2023-07-28_09:47:49, Step 399/1248 in Ep 22, LR: [2.9e-05, 2.9e-05] Time: 1.99s , train_loss: 1.8173, 
['0.0000']
2023-07-28_09:51:10, Step 499/1248 in Ep 22, LR: [2.9e-05, 2.9e-05] Time: 1.98s , train_loss: 1.8205, 
['0.0000']
2023-07-28_09:54:30, Step 599/1248 in Ep 22, LR: [2.8e-05, 2.8e-05] Time: 2.22s , train_loss: 1.8396, 
['0.0000']
2023-07-28_09:57:50, Step 699/1248 in Ep 22, LR: [2.8e-05, 2.8e-05] Time: 1.97s , train_loss: 1.8527, 
['0.0000']
2023-07-28_10:01:10, Step 799/1248 in Ep 22, LR: [2.8e-05, 2.8e-05] Time: 1.91s , train_loss: 1.8675, 
['0.0000']
2023-07-28_10:04:29, Step 899/1248 in Ep 22, LR: [2.7e-05, 2.7e-05] Time: 2.03s , train_loss: 1.8668, 
['0.0000']
2023-07-28_10:07:55, Step 999/1248 in Ep 22, LR: [2.7e-05, 2.7e-05] Time: 2.11s , train_loss: 1.8756, 
['0.0000']
2023-07-28_10:11:17, Step 1099/1248 in Ep 22, LR: [2.7e-05, 2.7e-05] Time: 1.94s , train_loss: 1.8864, 
['0.0000']
2023-07-28_10:14:38, Step 1199/1248 in Ep 22, LR: [2.7e-05, 2.7e-05] Time: 1.92s , train_loss: 1.8993, 
['0.0000']
2023-07-28_10:16:15, Step 1247/1248 in Ep 22, LR: [2.6e-05, 2.6e-05] Time: 1.97s , train_loss: 1.8993, 
['0.0000']
Epoch 22, LR 3.0418399668886506e-05, Train_Time 2519.33s, Loss: 1.8993
['0.0000']
Evaluation on train set, train losses 1.8992621298306263
 ma: 0.9358, label_f1: 0.8754, pos_recall: 0.8762 , neg_recall: 0.9954 
 Acc: 0.9734, Prec: 0.9839, Rec: 0.9880, F1: 0.9860
Evaluation on test set, valid losses 1.6108812005519868
 ma: 0.9364, label_f1: 0.8772, pos_recall: 0.8763 , neg_recall: 0.9965 
 Acc: 0.9774, Prec: 0.9868, Rec: 0.9894, F1: 0.9881
2023-07-28_10:17:39
------------------------------------------------------------
2023-07-28_10:21:21, Step 99/1248 in Ep 23, LR: [2.6e-05, 2.6e-05] Time: 1.96s , train_loss: 1.5619, 
['0.0000']
2023-07-28_10:24:42, Step 199/1248 in Ep 23, LR: [2.6e-05, 2.6e-05] Time: 1.99s , train_loss: 1.5461, 
['0.0000']
2023-07-28_10:28:06, Step 299/1248 in Ep 23, LR: [2.5e-05, 2.5e-05] Time: 2.03s , train_loss: 1.5714, 
['0.0000']
2023-07-28_10:32:04, Step 399/1248 in Ep 23, LR: [2.5e-05, 2.5e-05] Time: 3.12s , train_loss: 1.5927, 
['0.0000']
2023-07-28_10:36:49, Step 499/1248 in Ep 23, LR: [2.5e-05, 2.5e-05] Time: 3.03s , train_loss: 1.5964, 
['0.0000']
2023-07-28_10:41:31, Step 599/1248 in Ep 23, LR: [2.5e-05, 2.5e-05] Time: 2.70s , train_loss: 1.5978, 
['0.0000']
2023-07-28_10:46:09, Step 699/1248 in Ep 23, LR: [2.4e-05, 2.4e-05] Time: 2.80s , train_loss: 1.6105, 
['0.0000']
